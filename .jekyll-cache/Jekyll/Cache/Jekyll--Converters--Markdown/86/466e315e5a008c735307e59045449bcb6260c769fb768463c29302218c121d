I"P<p>Paper link: <a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></p>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>DQNì˜ continuous action domain ë²„ì „</li>
  <li>Off policy Actor-critic ì‚¬ìš©</li>
  <li>Deterministic Policy Gradient ì‚¬ìš©</li>
  <li>ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì•Œê³ ë¦¬ì¦˜ì´ dynamicsì— ëŒ€í•œ full observabilityë¥¼ ê°€ì§€ê³  ìˆëŠ” planning ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„ìŠ·í•  ì •ë„ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„.</li>
  <li>Raw pixel inputì„ ë°›ì•„ì„œ Policyë¥¼ end-to-end í•™ìŠµ ê°€ëŠ¥</li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>DQNì€ ì´ì‚°í™”ë˜ê³  low-dimensional action spaceë¥¼ ê°€ì§„ ë¬¸ì œë§Œ í’€ ìˆ˜ ìˆì—ˆìŒ. ì™œëƒí•˜ë©´ DQNì˜ policy í•™ìŠµ ìì²´ê°€ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ë¥¼ ìµœëŒ€í™” í•˜ëŠ” actionì„ ì°¾ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë£¨ì–´ì¡Œê¸° ë•Œë¬¸.</li>
  <li>ìœ„ ê°™ì€ ì ì´ ì™œ continuous domainì—ì„œ ì ìš©ì´ ë¶ˆê°€ëŠ¥í•œê°€?
    <ul>
      <li>ì¼ë‹¨ continuous domainì„ ì´ì‚°í™”ì‹œí‚¤ë ¤ë©´ ë¬´ìˆ˜íˆ ë§ì€ action spaceë¥¼ ê³ ë ¤í•´ì•¼í•¨. ì´ë ‡ê²Œ ë˜ë©´ dimension í•˜ë‚˜ê°€ ëŠ˜ì–´ë‚  ë•Œë§ˆë‹¤ ê³ ë ¤í•´ì•¼í•˜ëŠ” action spaceì˜ ê°¯ìˆ˜ê°€ exponentialí•˜ê²Œ ëŠ˜ì–´ë‚˜ì„œ 
  curse of dimensionality ë¬¸ì œë¥¼ ê²ªê²Œ ë¨.</li>
    </ul>
  </li>
  <li>DQNì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ë“¤ì„ ì±„íƒí•´ì„œ actor-critic methodë¥¼ stabilizeí•˜ê³ ì í•¨
    <ol>
      <li>Off-policyë¡œ ëª¨ì€ sampleë“¤ì„ ëª¨ì•„ì„œ replay buffer ë§Œë“¬. ì´ëŸ´ ê²½ìš° ì—¬ëŸ¬ ì—í”¼ì†Œë“œì— ê±¸ì³ ëª¨ì€ sampleë“¤ì„ í•™ìŠµì— ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— sample ê°„ì˜ correlationì„ ìµœëŒ€í•œ ì¤„ì¼ ìˆ˜ ìˆìŒ.</li>
      <li>Target Q-networkë¥¼ ì‚¬ìš©.</li>
      <li>DQNì—ì„œ ì‚¬ìš©í•œ íŠ¸ë¦­ë“¤ ì™¸ì—ë„ batch normalizationë„ ì‚¬ìš©</li>
    </ol>
  </li>
  <li>DDPGëŠ” ë™ì¼í•œ hyperparameter setê³¼ network structureë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í’ˆ.</li>
</ul>

<h2 id="background">Background</h2>

<ol>
  <li>ì¼ë°˜ì ì¸ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/45442859/131423574-f27e7d12-a97c-4c30-8950-0dd6a8d18f4a.png" alt="image" /></p>

<ol>
  <li>Bellman Equationì„ ì‚¬ìš©í•˜ì—¬ recursiveí•œ formìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/45442859/131423635-49ddd9c7-24de-4093-ad74-52331d2aa40a.png" alt="image" /></p>

<ol>
  <li>ë§Œì•½ policyê°€ deterministic í•˜ë‹¤ë©´ ë” ì´ìƒ a<sub>t+1</sub>ì— ë”°ë¥¸ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ì˜ ê¸°ëŒ“ê°’ì„ ê³„ì‚° í•˜ì§€ ì•Šì•„ë„ ë¨. ê¸°ëŒ“ê°’ì„ êµ¬í•˜ëŠ” ê³¼ì •ì€ íŠ¹ì • actionì˜ í™•ë¥ ê³¼ í•´ë‹¹ actionì„ ì·¨í–ˆì„ ë•Œì˜ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ë¥¼ ê³±í•´ì„œ ëª¨ë‘ ë”í•˜ëŠ”ë°, actionì´ ê²°ì •ì ì´ë©´ í•´ë‹¹ actionì— ëŒ€í•œ í–‰ë™ê°€ì¹˜í•¨ìˆ˜ë§Œ ê³ ë ¤í•˜ë©´ ë¨.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/45442859/131431845-f3d5f088-8544-4b8f-b380-cb53ffb364be.png" alt="image" /></p>

<ul>
  <li>ì´ë ‡ê²Œ ë˜ë©´ Q<sup>Î¼</sup>ë¥¼ stochasitc behavior policy Î²ë¥¼ í†µí•´ ì–»ì€ sampleë“¤ì„ í†µí•´ off-policy í•™ìŠµí•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<ol>
  <li>Off-policy ì•Œê³ ë¦¬ì¦˜ì˜ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ë¡œ Q-Learningì„ ì–¸ê¸‰í•˜ê³  ìˆê¸°ë„ í•˜ë‹¤.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/45442859/131432478-50fe47e6-d1f4-4c90-9a1c-c8d5f03e19ad.png" alt="image" /></p>

<h2 id="algorithm">Algorithm</h2>

<ul>
  <li>Actor-critic approach based on the DPG algorithm</li>
  <li>DPG ì•Œê³ ë¦¬ì¦˜ì€ actor function Î¼(sIÎ¸<sup>Î¼</sup>)ì„ ì‚¬ìš©í•˜ì—¬ stateë¥¼ íŠ¹ì • actionìœ¼ë¡œ deterministically mapping í•œë‹¤.</li>
  <li>Actor functionì€ ë‹¤ìŒê³¼ ê°™ì€ policy gradient ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ update í•œë‹¤.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/45442859/131443050-9fb4d468-528e-487e-b6f0-0923f6e17f57.png" alt="image" /></p>

<ul>
  <li>Criticì€ Bellman equationì„ ì‚¬ìš©í•œ Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ í•™ìŠµí•œë‹¤.</li>
  <li>ëŒ€ë¶€ë¶„ì˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ ê·¸ë ‡ë“¯ neural networkë¥¼ ê°•í™”í•™ìŠµì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” sampleë“¤ì´ independently and identically distributed ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì´ í•„ìš”í•˜ë‹¤.
    <ul>
      <li>DQNì€ replay bufferë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í–ˆë‹¤.</li>
      <li>time stepë§ˆë‹¤ minibatchë¥¼ ìƒ˜í”Œë§í•˜ì—¬ actorì™€ criticì„ ì—…ë°ì´íŠ¸ í–ˆë‹¤.</li>
    </ul>
  </li>
  <li>Q-Learningì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ì—…ë°ì´íŠ¸ í•˜ê³  ìˆëŠ” Q functionì´ target networkë¡œë„ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— Q functionì´ divergeí•  ìˆ˜ë„ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.
    <ul>
      <li>ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Q í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•´ì„œ target networkë¥¼ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼ â€œsoftâ€ target updateë¥¼ ì‚¬ìš©í•œë‹¤.</li>
      <li>Soft updateëŠ” Î¸â€™ &lt;- Ï„Î¸ + (1-Ï„)Î¸â€™ with Ï„ Â«Â 1ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ë° ì—¬ê¸°ì„œ Î¸â€˜ëŠ” ì—…ë°ì´íŠ¸ ì „ì˜ actorì™€ criticì˜ parameterì´ë‹¤. ì¦‰ ì—…ë°ì´íŠ¸ ì „ê³¼ í›„ì˜ parameterë¥¼ ì ì ˆíˆ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ parameterë¥¼ ì–»ëŠ”ë‹¤ëŠ” ëœ»ì´ë‹¤.</li>
      <li>ì´ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ updateë¥¼ ì²œì²œíˆ, ê·¸ë¦¬ê³  ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆë‹¤. ì—…ë°ì´íŠ¸ê°€ ì²œì²œíˆ ì§„í–‰ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ, ì•ˆì •ì„± ì¸¡ë©´ì—ì„œ ê·¸ë§Œí¼ì˜ íš¨ê³¼ë¥¼ ë‚´ê³  ìˆë‹¤ê³  ì„¤ëª…í•˜ê³  ìˆë‹¤.</li>
    </ul>
  </li>
  <li>Low-dimensional feature vector observationì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ê° observationì´ unitì´ ë‹¤ë¥´ê±°ë‚˜ scaleì´ ë‹¤ë¥¸ ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ batch normalizationì„ ì‚¬ìš©í•œë‹¤.
    <ul>
      <li>Minibatchì˜ sampleë“¤ì´ unit mean and varianceë¥¼ ê°€ì§€ë„ë¡ normalize</li>
      <li>State input, all layers of Î¼, all layers of Q networkì— normalizationì„ ì§„í–‰</li>
    </ul>
  </li>
  <li>Continuous dimensionì—ì„œ ê°€ì¥ í° ë¬¸ì œëŠ” explorationì´ë‹¤.
    <ul>
      <li>ê¸°ì¡´ì˜ actor policyì— noiseë¥¼ ì¶”ê°€í•´ì¤Œìœ¼ë¡œì¨ explorationì´ ê°€ëŠ¥í•˜ë‹¤.<br />
<img src="https://user-images.githubusercontent.com/45442859/131446819-45aaa56d-32fe-493f-9ff5-4570b9bae560.png" alt="image" /></li>
      <li>DDPGì—ì„œëŠ” Ornstein-Uhlenbeck processë¥¼ ì‚¬ìš©í•´ì„œ noiseë¥¼ sample í–ˆë‹¤.</li>
    </ul>
  </li>
</ul>

<h2 id="pseudocode">Pseudocode</h2>
<p><img src="https://user-images.githubusercontent.com/45442859/131446937-d9b5f16f-d2e7-43f4-8c1f-360a927cba92.png" alt="image" /></p>

<h2 id="results">Results</h2>

<ul>
  <li>Low-dimensional state description (joint angles and positions)</li>
  <li>High-dimensional renderings of the environment</li>
  <li>For each timestep, step the simulation 3 timesteps, rendering each time.</li>
  <li>Observation reported to the agent contains 9 feature maps (RGP of each of the 3 renderings) which allows to infer velocieis using the differences between frames.</li>
  <li>Frames were downsampled to 64 x 64 pixels and the 8-bit RGB values were converted to floating point scaled to [0,1]</li>
  <li>Test ì‹œì—ëŠ” explorating noiseë¥¼ ì œì™¸í•œ policyë¥¼ ì‚¬ìš©.</li>
</ul>
:ET