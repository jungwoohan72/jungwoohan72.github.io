I"‰ <p>Paper Link: <a href="https://arxiv.org/abs/1803.08475">https://arxiv.org/abs/1803.08475</a><br />
Source Code (Official): <a href="https://github.com/wouterkool/attention-learn-to-route">https://github.com/wouterkool/attention-learn-to-route</a></p>

<h1 id="abstract">Abstract</h1>

<ol>
  <li>ì¡°í•©ìµœì í™” ë¬¸ì œë¥¼ í’€ ë•Œ ê¸°ì¡´ì˜ optimal solutionì„ êµ¬í•˜ëŠ” ê³¼ì •ì´ ì•„ë‹Œ, heuristicì„ ì‚¬ìš©í•˜ì—¬ sub-optimal í•˜ì§€ë§Œ computational cost
ì¸¡ë©´ì—ì„œ ì´ì ì„ ê°€ì§€ëŠ” solutionì„ ì°¾ê³ ì í•˜ëŠ” ì•„ì´ë””ì–´ê°€ ì œì‹œë¨.</li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Pointer Networkì—ì„œ ì‚¬ìš©ëœ attention ê°œë…ì„ í† ëŒ€ë¡œ ì¡°í•© ìµœì í™” ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ëª¨ë¸ì„ ì œì‹œí•˜ê³  REINFORCE ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ í•™ìŠµ ì‹œí‚¤ëŠ”
ë°©ë²•ì„ ì œì‹œ.</li>
  <li>Travelling Salesman Problem (TSP), Vehicle Routing Problem (VRP), Orienteering Problem (OP), Prize Collecting TSP (PCTSP)
ë“± ì—¬ëŸ¬ routing ë¬¸ì œë¥¼ í•˜ë‚˜ì˜ hyperparameterë¡œ í’€ì–´ì„œ ì œì‹œí•œ ëª¨ë¸ì˜ generalizabilityë¥¼ ê°•ì¡°.</li>
</ol>

<h1 id="what-is-combinatorial-optimization-ì¡°í•©ìµœì í™”">What is Combinatorial Optimization (ì¡°í•©ìµœì í™”)?</h1>

<p>Definition: Process of searching for maxima (or minima) of an objective function F whose domain is discrete but
large configuration space (as opposed to an N-dimensional continuous space)</p>

<p>ì‰½ê²Œ ë§í•˜ë©´ objective functionì„ ìµœëŒ€í™” í•˜ê±°ë‚˜ loss functionì„ ìµœì†Œí™”í•˜ëŠ” ì¡°í•©ì„ ì°¾ì•„ë‚´ëŠ” ê²Œ ëª©ì ì¸ ì—°êµ¬ë¶„ì•¼.</p>

<p>ëŒ€í‘œì ìœ¼ë¡œ TSPê°€ ìˆëŠ”ë°, traveling distanceë‚˜ traveling timeì„ ìµœì†Œí™” í•˜ë©´ì„œ ì£¼ì–´ì§„ nodeë“¤ì„ í•œë²ˆì”©ë§Œ ë°©ë¬¸í•˜ê³  ì¶œë°œì§€ì ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ë¬¸ì œì´ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/45442859/128311518-2d3cff43-ec1e-4ca9-9eae-903d25762afb.png" alt="image" /></p>

<p>ì¡°í•©ìµœì í™”ì— ì ‘ê·¼í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆëŠ”ë°, exact solution, ì¦‰ optimalí•œ solutionì„ ì°¾ê¸° ìœ„í•œ ë°©ë²•ì´ ìˆê³ , heuristicì„ ì‚¬ìš©í•˜ì—¬ sub-optimalí•˜ì§€ë§Œ
computational cost ì¸¡ë©´ì—ì„œ ì´ì ì„ ê°€ì§€ëŠ” solutionì„ ì°¾ê¸° ìœ„í•œ ë°©ë²•ì´ ìˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ë°©ë²•ë¡ ì€ í›„ìì— ì†í•˜ëŠ” ì ‘ê·¼ ë°©ë²•ìœ¼ë¡œ, RLì„ ì‚¬ìš©í•˜ì˜€ë‹¤.</p>

<p><img src="https://user-images.githubusercontent.com/45442859/128312683-4dabaa4f-13e6-48cc-9801-a1b0ac86ff77.png" alt="image" /></p>

<p>ì €ìëŠ” ì¡°í•©ìµœì í™” ë¬¸ì œë¡œ ë¶„ë¥˜ë˜ëŠ” ì—¬ëŸ¬ routing ë¬¸ì œë“¤ì„ í’€ê¸° ìœ„í•´ ì´ ëª¨ë¸ì„ ì œì‹œí–ˆëŠ”ë°, ì´ ëª¨ë¸ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ TSP problem settingì„ ì´ìš©í•œë‹¤ê³  í•œë‹¤.
ëª¨ë¸ì´ TSP ë¬¸ì œë§Œì„ í’€ ìˆ˜ ìˆëŠ”ê±´ ì•„ë‹ˆë©°, ë¬¸ì œ ì„¸íŒ…ë§ˆë‹¤ ì•½ê°„ì˜ ëª¨ë¸ ìˆ˜ì •ì´ë‚˜ í™˜ê²½ ì„¸íŒ…ì„ í•´ì¤Œìœ¼ë¡œì¨ ë‹¤ì–‘í•œ routing ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.</p>

<h1 id="problem-setting">Problem Setting</h1>

<ul>
  <li>Problem instance <strong>s</strong> as a graph with <strong>n</strong> nodes, which are fully connected (including self-connections)</li>
  <li>Each node is represented by feature <strong>x<sub>i</sub></strong> which is coordinate of node <strong>i</strong></li>
  <li>Solution is defined as a permutation of nodes <strong>Ï€</strong> = (Ï€<sub>1</sub>,â€¦,Ï€<sub>n</sub>) where 
Ï€<sub>t</sub> â‰  Ï€<sub>tâ€™</sub></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Stochastic policy for choosing next node p<sub>Î¸</sub>(<strong>Ï€</strong></td>
          <td>s) = <strong>âˆ<sub>t=1</sub></strong> p<sub>Î¸</sub>(Ï€<sub>t</sub></td>
          <td>s,<strong>Ï€</strong><sub>1:t-1</sub>)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h1 id="attention-model">Attention Model</h1>

<ul>
  <li>Encoder produces embeddings of all input nodes.</li>
  <li>Decoder produces the sequence <strong>Ï€</strong> of input nodes, one node at a time. Also, the decoder observes a mask to know which nodes have been visited.</li>
</ul>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128447036-ce112ed4-3a59-472d-ba62-e5ffe31c7025.png" alt="attention" width="75%" height="75%" />
</p>

<p>Attentionì€ seq-to-seq ëª¨ë¸ì— ë§ì´ ì“°ì´ëŠ”ë°, í•œ ë¬¸ì¥ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì˜ˆê°€ ëŒ€í‘œì ì´ë‹¤. ì¦‰ íŠ¹ì • ë‹¨ì–´ë¥¼ outputìœ¼ë¡œ ë‚´ê¸° ìœ„í•´ ì–´ë–¤ input ë‹¨ì–´ë“¤ì— â€œì§‘ì¤‘â€ í•  ê²ƒì¸ì§€ ê²°ì •í•˜ëŠ” ê²Œ attention mechanismì´ë¼ê³  
ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.</p>

<h2 id="encoder">Encoder</h2>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128447359-d68e4783-8ddc-4522-95e3-d6a31f3d6863.png" alt="encoder" width="75%" height="75%" />
</p>

<ul>
  <li>Inputì€ ê° ë…¸ë“œì˜ ì¢Œí‘œ (2-dimensional)</li>
  <li>Outputì€ ì—¬ëŸ¬ Multi-Head-Attention layerë¥¼ ê±°ì¹œ embedding vector (128-dimensional)</li>
  <li>ê° ë…¸ë“œì˜ embeddingê³¼ ë”ë¶ˆì–´ ë…¸ë“œë“¤ì˜ í‰ê· ì„ ë‚¸ aggregated embeddingë„ outputìœ¼ë¡œ ë‚´ì¤Œ.</li>
</ul>

<h3 id="ê°-attention-layerëŠ”-ì•„ë˜ì™€-ê°™ì´-êµ¬ì„±">ê° Attention layerëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±</h3>

<ol>
  <li>ì¼ë‹¨ Raw Inputì´ MLPë¥¼ ê±°ì¹˜ê³  ë‚˜ë©´ 128-dimensional Embeddingì´ ë§Œë“¤ì–´ì§. (ì²«ë²ˆì§¸ ì´ˆë¡ìƒ‰ í™”ì‚´í‘œ)</li>
  <li>Embeddingì— Weight matrixë¥¼ ê³±í•´ì„œ (query, key, value) setì„ ë§Œë“¬. Multi-Head Attentionì´ë¼ê³  ë¶ˆë¦¬ìš°ëŠ” ì´ìœ ëŠ” ì¢€ ë” ë‹¤ì–‘í•œ featureë“¤ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ (query, key, value) setì„ ìƒì„±í•  ë•Œ 
dimensionì„ ìª¼ê°œê¸° ë•Œë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ Single Head Attentionìœ¼ë¡œ 128x128 weight matrixë¥¼ ì‚¬ìš©í•´ 128-dimensional vectorë¡œ project í•´ì£¼ëŠ” ëŒ€ì‹ ì— 8ê°œì˜ 16x128 weight matrixë¥¼ ì‚¬ìš©í•´ì„œ 16-dimensional vector 8ê°œë¥¼ ë§Œë“¤ì–´
ë‚˜ì¤‘ì— í•©ì¹œë‹¤.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448124-29776d0f-6f63-42c8-a1b1-8383469d0063.png" alt="query" width="50%" height="50%" />
</p>

<ol>
  <li>ê¸°ì¤€ì´ ë˜ëŠ” nodeì˜ queryì™€ ë‚˜ë¨¸ì§€ ì£¼ë³€ nodeë“¤ì˜ keyë¼ë¦¬ dot-productë¥¼ í•´ì¤˜ì„œ compatibilityë¥¼ ê³„ì‚°. ì˜ˆë¥¼ ë“¤ë©´, 1ë²ˆ ë…¸ë“œì—ê²Œ ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì´ ì–¼ë§ˆë‚˜ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ê°€ í•˜ëŠ”
ì ìˆ˜ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” ê³¼ì •. ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ” nodeì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448677-58382d71-5595-4249-a494-8106ec025a9b.png" alt="MHA" width="75%" height="75%" />
</p>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448736-5aa89b09-1dc6-4d0e-b037-bacb7d209352.png" alt="u" width="50%" height="50%" />
</p>

<ol>
  <li>ê³„ì‚°ëœ compatibilityì— softmax functionì„ ì”Œì›Œì„œ normalize ì‹œì¼œì¤€ ê°’ì„ attention scoreë¡œ ì”€.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448860-8dc3d6a9-d875-4640-8118-067328e00cb2.png" alt="a" width="25%" height="25%" />
</p>

<ol>
  <li>ê° attention scoreëŠ” ê° ë…¸ë“œì˜ value vectorì™€ ê³±í•´ì ¸ì„œ ì „ë¶€ ë”í•´ì§.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448936-b8c8e0f6-c512-435a-b052-bbbc978bc3db.png" alt="h" width="25%" height="25%" />
</p>

<ol>
  <li>Multi-Head Attentionì¸ ê²½ìš° ìœ„ì˜ hâ€™<sub>i</sub> vectorëŠ” 16x1ì˜ í¬ê¸°ë¥¼ ê°€ì§„ë‹¤. ì•ì—ì„œ ë§í–ˆë“¯ì´ ì´ ê°™ì€ 8ê°œì˜ vectorì— 128x16 weight matrixë¥¼ ê³±í•´ì£¼ì–´ ëª¨ë‘ ë”í•´ì„œ ìµœì¢…ì ìœ¼ë¡œ
128x1 Embedding vectorë¥¼ ë§Œë“¤ì–´ ë‚¸ë‹¤.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128452206-936ed6eb-f3d5-413f-a2cc-4d7f1e98a835.png" alt="MHA_sig" width="75%" height="75%" />
</p>

<p>ìœ„ ê³¼ì •ì´ í•˜ë‚˜ì˜ Attention layerì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ì´ë‹¤.</p>

<h3 id="attention-layerë¥¼-í†µê³¼í•˜ê³ -ë‚œ-ë‹¤ìŒì˜-feed-forward-layerëŠ”-ë‹¨ìˆœí•˜ê²Œ-reluì™€-batch-normalizationìœ¼ë¡œ-ì´ë£¨ì–´ì§">Attention Layerë¥¼ í†µê³¼í•˜ê³  ë‚œ ë‹¤ìŒì˜ Feed-Forward LayerëŠ” ë‹¨ìˆœí•˜ê²Œ ReLUì™€ Batch Normalizationìœ¼ë¡œ ì´ë£¨ì–´ì§.</h3>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128452744-f8bc1fb8-be4e-40f4-9e4d-d10641048b59.png" alt="BN" width="75%" height="75%" />
</p>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128452912-436ec81e-11eb-4c53-b978-3ec542a2e70a.png" alt="FF" width="75%" height="75%" />
</p>

<h2 id="decoder">Decoder</h2>
:ET