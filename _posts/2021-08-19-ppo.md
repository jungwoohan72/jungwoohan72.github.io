---
layout: post
title: Proximal Policy Optimiztion (PPO)
categories:
  - RL Algorithm Replication
tags:
  - RL
  - Policy gradient
  - PPO
---

Paper Link: [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)

* Policy Gradient 방법 중 하나로 실험 결과에서 대부분의 다른 Policy Gradient 보다 좋은 성능을 보임.
* Sthocastic gradient ascent
* 다른 Policy gradient 알고리즘들은 minibatch 하나당 한번의 gradient 업데이트를 하고 끝내는 반면 PPO는 minibatch 하나를 여러번의 epoch에 걸쳐 사용하여 gradient를 업데이트 함.
* TRPO에 비해서 구현이 비교적 쉬움.

# 다른 Policy Gradient에 비해 나은 점?

* Vanilla Policy Gradient 방법들은 data efficiency 측면과 안정성 측면에서 좋지 않은 모습을 보임.
* TRPO 같은 경우는 내용이 너무 복잡함.
* TRPO와 달리 first-order optimization을 통해 gradient update를 진행함.

# Policy Optimization이란?

* Advantageous Actor-Critic의 loss function은 다음과 같음. Advantage function은 보통 Q(s,a)-V(s)이다. 아래 loss function을 maximize하는 방향으로 학습이 진행.
왜 minimize가 아니고 maximize하는지 모르겠다면 REINFORCE 게시물 참조. 간단하게 말하면 아래 loss function은 value function과 같기 때문에 최대로 만들어줘야 한다.

    ![image](https://user-images.githubusercontent.com/45442859/130087533-d6a94f79-c982-4cd6-8f63-2df0d1cc8b0d.png)


* 위 loss function을 따라서 on-policy 업데이트를 계속 진행하면 REINFORCE 알고리즘에서 살펴 봤듯이 학습이 굉장히 불안정하다. 논문에서는 
'destructively large policy updates'라고 표현하고 있는데, gradient가 너무 급격하게 바뀌는 경우가 많기 때문인 것 같다.
  
# TRPO에서 차용한 loss function

TRPO의 loss function ('surrogate' objective)

![image](https://user-images.githubusercontent.com/45442859/130090051-b53fb5ba-1959-4806-9c41-ed6ce695dd2b.png)

결과론적으로 위 loss function에 gradient를 취해준 값은 Advantageous Actor-Critic의 loss function에 gradient를 취해준 값과 같은데, importance sampling을 통해 같음을 증명할 수 있다.

![image](https://user-images.githubusercontent.com/45442859/130090951-96fbd280-872f-4644-98d0-718224988f90.png)

Importance sampling은 위와 같다. 간단히 말해서 P 분포에서 sampling한 x를 input으로 가지는 f 함수의 기댓값은 P와 다른 Q 분포에서 x를 sampling 함으로써 
구할 수 있다는 것이다.

컨셉은 이렇고, 직접적으로 왜 같은지는 아래에서 확인할 수 있다.

![image](https://user-images.githubusercontent.com/45442859/130091876-81fbafe7-b26b-4612-8be7-e4c8115bea23.png)

# KL이 무엇인가?

loss function에 붙은 constraint를 살펴보면 KL이라는 것을 볼 수 있다. KL은 Kullback-Leibler divergence로 두 확률분포의 차이를 계산하는 데에 사용하는 함수이다.
즉, 업데이트 전의 &theta;<sub>old</sub>와 업데이트 후의 &theta;의 차이나는 정도를 제한함으로써 급격한 변화를 막겠단 뜻이다. 하지만 위와 같은 constraint가 걸린 optimization 문제는
풀기에 상당히 복잡하므로 TRPO에서는 constraint form 대신 아래와 같은 penalty form을 사용해서 optimization을 진행하는 것을 제시했다.

![image](https://user-images.githubusercontent.com/45442859/130375721-21e07832-c99b-434f-8db8-169d40cf5801.png)

하지만 TRPO는 위와 같은 penalty form 대신에 hard constraint form을 사용했는데, 이는 다양한 문제에 적용 가능한 하나의 &beta;를 찾기 어려울 뿐 아니라 하나의 문제에서도 고정된 &beta; 값은 성능이
좋지 못한 것을 발견했다. 

# Clipped Surrogate Objective

위와 같은 문제를 해결하기 위해 PPO에서는 KL divergence를 쓰는 대신에 Clipped Surrogate Objective라는 modified된 loss 함수를 제시했다. 

![image](https://user-images.githubusercontent.com/45442859/130376120-7daa3180-6fc7-49ea-8817-f43bb6955e82.png)

여기서 r<sub>t</sub>(&theta;)는 다음과 같다.

![image](https://user-images.githubusercontent.com/45442859/130376377-5ee7bf08-32cf-417b-b1ef-7218c12c206a.png)

만약 &epsilon; = 0.2 라고 한다면 clip(r<sub>t</sub>(&theta;), 1-&epsilon;, 1+&epsilon;)은 항상 0.8에서 1.2 사이 값을 가지게 된다. 즉 업데이트 전 policy와 업데이트 후 policy의 probability ratio를 일정 범위 안에 속하도록 고정하겠다는 의미이다.
이렇게 clip된 값과 원래의 r<sub>t</sub>(&theta;) 값 중 더 작은 값을 쓰도록 하여 update가 너무 급격하게 일어나지 않게 만들어준다.

![image](https://user-images.githubusercontent.com/45442859/130378700-cca4afbf-86e2-41a7-bcdb-379ed51d2496.png)

위와 같은 clipped surrogate objective를 사용하여 update하는 과정을 살펴보면 아래와 같다. 우선 advantage가 0보다 크다는 의미는 해당 action이 좋은 action이므로 다음 번에는 해당 action을 할 확률을 올려야 된다는 소리이다.