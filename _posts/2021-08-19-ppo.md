---
layout: post
title: Proximal Policy Optimiztion (PPO)
categories:
  - RL Algorithm Replication
tags:
  - RL
  - Policy gradient
  - PPO
---

Paper Link: [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)

* Policy Gradient 방법 중 하나로 실험 결과에서 대부분의 다른 Policy Gradient 보다 좋은 성능을 보임.
* Sthocastic gradient ascent
* 다른 Policy gradient 알고리즘들은 minibatch 하나당 한번의 gradient 업데이트를 하고 끝내는 반면 PPO는 minibatch 하나를 여러번의 epoch에 걸쳐 사용하여 gradient를 업데이트 함.
* TRPO에 비해서 구현이 비교적 쉬움.

# 다른 Policy Gradient에 비해 나은 점?

* Vanilla Policy Gradient 방법들은 data efficiency 측면과 안정성 측면에서 좋지 않은 모습을 보임.
* TRPO 같은 경우는 내용이 너무 복잡함.
* TRPO와 달리 first-order optimization을 통해 gradient update를 진행함.

# Policy Optimization이란?

* Advantageous Actor-Critic의 loss function은 다음과 같음. Advantage function은 보통 Q(s,a)-V(s)이다. 아래 loss function을 maximize하는 방향으로 학습이 진행.
왜 minimize가 아니고 maximize하는지 모르겠다면 REINFORCE 게시물 참조. 간단하게 말하면 아래 loss function은 value function과 같기 때문에 최대로 만들어줘야 한다.

![image](https://user-images.githubusercontent.com/45442859/130087533-d6a94f79-c982-4cd6-8f63-2df0d1cc8b0d.png)

* 위 loss function을 따라서 on-policy 업데이트를 계속 진행하면 REINFORCE 알고리즘에서 살펴 봤듯이 학습이 굉장히 불안정하다. 논문에서는 
'destructively large policy updates'라고 표현하고 있는데, gradient가 너무 급격하게 바뀌는 경우가 많기 때문인 것 같다.
  
# TRPO에서 차용한 loss function

* TRPO의 loss function ('surrogate' objective)

![image](https://user-images.githubusercontent.com/45442859/130090051-b53fb5ba-1959-4806-9c41-ed6ce695dd2b.png)

결과론적으로 위 loss function에 gradient를 취해준 값은 Advantageous Actor-Critic의 loss function에 gradient를 취해준 값과 같은데, importance sampling을 통해 같음을 증명할 수 있다.

![image](https://user-images.githubusercontent.com/45442859/130090951-96fbd280-872f-4644-98d0-718224988f90.png)

Importance sampling은 위와 같다. 간단히 말해서 P 분포에서 sampling한 x를 input으로 가지는 f 함수의 기댓값은 P와 다른 Q 분포에서 x를 sampling 함으로써 
구할 수 있다는 것이다.

컨셉은 이렇고, 직접적으로 왜 같은지는 아래에서 확인할 수 있다.

![image](https://user-images.githubusercontent.com/45442859/130091876-81fbafe7-b26b-4612-8be7-e4c8115bea23.png)

* KL이 무엇인가?

loss function에 붙은 constraint를 살펴보면 KL이라는 것을 볼 수 있다. KL은 