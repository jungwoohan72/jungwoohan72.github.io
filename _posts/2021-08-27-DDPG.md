---
layout: post
title: Continuous Control with Deep Reinforcement Learning
categories:
  - RL Algorithm Replication
tags:
  - RL
  - Policy gradient
  - DDPG
---

Paper link: [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)

## Abstract

* DQN의 continuous action domain 버전
* Off policy Actor-critic 사용
* Deterministic Policy Gradient 사용
* 이 논문에서 제시한 알고리즘이 dynamics에 대한 full observability를 가지고 있는 planning 알고리즘과 비슷할 정도로 좋은 성능을 보임.
* Raw pixel input을 받아서 Policy를 end-to-end 학습 가능

## Introduction

* DQN은 이산화되고 low-dimensional action space를 가진 문제만 풀 수 있었음. 왜냐하면 DQN의 policy 학습 자체가 행동가치함수를 최대화 하는 action을 찾는 방향으로 이루어졌기 때문.
* 위 같은 점이 왜 continuous domain에서 적용이 불가능한가?
    * 일단 continuous domain을 이산화시키려면 무수히 많은 action space를 고려해야함. 이렇게 되면 dimension 하나가 늘어날 때마다 고려해야하는 action space의 갯수가 exponential하게 늘어나서 
    curse of dimensionality 문제를 겪게 됨.
      
* DQN에서 다음과 같은 장점들을 채택해서 actor-critic method를 stabilize하고자 함
    1. Off-policy로 모은 sample들을 모아서 replay buffer 만듬. 이럴 경우 여러 에피소드에 걸쳐 모은 sample들을 학습에 사용하기 때문에 sample 간의 correlation을 최대한 줄일 수 있음.
    2. Target Q-network를 사용.
    3. DQN에서 사용한 트릭들 외에도 batch normalization도 사용
    
* DDPG는 동일한 hyperparameter set과 network structure를 사용하여 여러 다양한 문제를 품.

## 