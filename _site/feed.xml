<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://jungwoohan72.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jungwoohan72.github.io/" rel="alternate" type="text/html" /><updated>2021-09-02T16:21:57+09:00</updated><id>https://jungwoohan72.github.io/feed.xml</id><title type="html">Jungwoo Han</title><subtitle>Welcome</subtitle><author><name>Andrew Fong</name></author><entry><title type="html">OpenAI gym tip</title><link href="https://jungwoohan72.github.io/blog/2021/09/01/openai-tips.html" rel="alternate" type="text/html" title="OpenAI gym tip" /><published>2021-09-01T00:00:00+09:00</published><updated>2021-09-01T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/blog/2021/09/01/openai-tips</id><content type="html" xml:base="https://jungwoohan72.github.io/blog/2021/09/01/openai-tips.html">&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;State와 Action Space 확인&lt;br /&gt;
s_dim = env.observation_space.shape[0]&lt;br /&gt;
a_dim = env.action_space.n&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;흠&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Andrew Fong</name></author><category term="Blog" /><summary type="html">State와 Action Space 확인 s_dim = env.observation_space.shape[0] a_dim = env.action_space.n</summary></entry><entry><title type="html">Conda 명령어</title><link href="https://jungwoohan72.github.io/blog/2021/08/31/conda-tips.html" rel="alternate" type="text/html" title="Conda 명령어" /><published>2021-08-31T00:00:00+09:00</published><updated>2021-08-31T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/blog/2021/08/31/conda-tips</id><content type="html" xml:base="https://jungwoohan72.github.io/blog/2021/08/31/conda-tips.html">&lt;ol&gt;
  &lt;li&gt;새로운 가상환경: conda create -m “env name” python=”python version”&lt;/li&gt;
  &lt;li&gt;가상환경 상속: conda create –name “new env name” –clone “old env name”&lt;/li&gt;
  &lt;li&gt;가상환경에 설치된 package list: conda list&lt;/li&gt;
  &lt;li&gt;가상환경 list: conda env list&lt;/li&gt;
  &lt;li&gt;가상환경 삭제: conda remove –name “env name” –all&lt;/li&gt;
  &lt;li&gt;패키지 다운로드: conda install -c conda-forge “package name”&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Andrew Fong</name></author><category term="Blog" /><summary type="html">새로운 가상환경: conda create -m “env name” python=”python version” 가상환경 상속: conda create –name “new env name” –clone “old env name” 가상환경에 설치된 package list: conda list 가상환경 list: conda env list 가상환경 삭제: conda remove –name “env name” –all 패키지 다운로드: conda install -c conda-forge “package name”</summary></entry><entry><title type="html">Multi-Agent Graph-Attention Communication and Teaming</title><link href="https://jungwoohan72.github.io/papers%20explained/2021/08/30/multi-agent-graph-attention-attention-communication-and-teaming.html" rel="alternate" type="text/html" title="Multi-Agent Graph-Attention Communication and Teaming" /><published>2021-08-30T00:00:00+09:00</published><updated>2021-08-30T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/papers%20explained/2021/08/30/multi-agent-graph-attention-attention-communication-and-teaming</id><content type="html" xml:base="https://jungwoohan72.github.io/papers%20explained/2021/08/30/multi-agent-graph-attention-attention-communication-and-teaming.html">&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1911.10715&quot;&gt;https://arxiv.org/abs/1911.10715&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;언제 다른 agent와 협력할 것인가?&lt;/li&gt;
  &lt;li&gt;어떤 agent와 협력할 것인가?&lt;/li&gt;
  &lt;li&gt;어떻게 받아온 message를 처리할 것인가?&lt;/li&gt;
  &lt;li&gt;Graph-attention을 기반으로 함.&lt;/li&gt;
  &lt;li&gt;Scheduler에서 어떤 agent와 언제 협력할 것인지를 결정. Encoder 단에서 differentiable attention을 사용하여 Message Processor에 differentiable graph를 제공하여 end-to-end 학습을 가능하게 함.&lt;/li&gt;
  &lt;li&gt;Message Processor에서 Graph Attention Network를 사용하여 message를 처리&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;A novel graph communication protocol that determines “when” and “whom” to communicate via an end-to-end framework.&lt;/li&gt;
  &lt;li&gt;Modeling the topology of connections between agents as a dynamic directed graph that accommodates time-varying communication needs and accurately captures the relations between agents.&lt;/li&gt;
  &lt;li&gt;언제 누구와 communicate할지에 대한 정보는 scheduler의 encoder 단에서 directed graph 형태로 가공된다.&lt;/li&gt;
  &lt;li&gt;Message processor는 graph attention network를 사용하여 받은 message와 directed graph를 가공한다.&lt;/li&gt;
  &lt;li&gt;가공된 메시지들은 각 agent의 policy 학습에 쓰인다.&lt;/li&gt;
  &lt;li&gt;언제 communicate해야 좋을지에 대한 논문은 있었지만, 누구와 communicate하는지 결정하지 않고 모든 agent와 communicate 했기 때문에 computation 측면에서 낭비가 있었다. -&amp;gt; Learning when to communicate at scale in multiagent cooperative and competitive tasks&lt;/li&gt;
  &lt;li&gt;특정 agent와 협력하는 논문도 있었지만 언제해야되는지에 대한 부분은 다루지 않았었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contribution&quot;&gt;Contribution&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Scheduler와 Message Processor로 이루어진 novel graph-attention communication protocol 제시&lt;/li&gt;
  &lt;li&gt;Differentiable graph를 다루기 위해 Message Processor에 GAT 적용. 이 덕분에 end-to-end 학습 가능.&lt;/li&gt;
  &lt;li&gt;성능 측면에서 Prior method 능가&lt;/li&gt;
  &lt;li&gt;3대2 축구 게임 세팅에서 실제 실험 진행&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Each agent observes other agents as part of the environment.&lt;/li&gt;
  &lt;li&gt;Difficult for each agent to deduce its own contribution to the team’s success&lt;/li&gt;
  &lt;li&gt;Many MARL algorithms have pursued centralized training and decentralized execution
    &lt;ul&gt;
      &lt;li&gt;Cooperative scheme without explicit communication channels through centralized critics&lt;/li&gt;
      &lt;li&gt;MADDPG, COMA&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MARL with communication
    &lt;ul&gt;
      &lt;li&gt;Agents communicate and exchange messages during execution.&lt;/li&gt;
      &lt;li&gt;DIAL builds up limited-bandwidth differentiable discrete communication channels&lt;/li&gt;
      &lt;li&gt;CommNet extends to a continuous communication protocol designed for fully cooperative tasks by receiving averaged encoded hidden states from other agents. In this case, there will be some information loss during averaging process.&lt;/li&gt;
      &lt;li&gt;IC3Net uses a gating mechanism to decide when to communicate, so it can be applied to competitive scenarios.&lt;/li&gt;
      &lt;li&gt;하지만 CommNet과 IC3Net 같이 hidden feature를 단순 평균을 내는 것은 성능 측면에서 좋지 못함.&lt;/li&gt;
      &lt;li&gt;TarMAC은 when to communicate와 whom to send message를 다루고 있지 않음.&lt;/li&gt;
      &lt;li&gt;ATOC와 SchedNet은 communication group을 manually configure한다는 단점이 있음.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MARL using GNN
    &lt;ul&gt;
      &lt;li&gt;DGN employs multi-head dot-production attention&lt;/li&gt;
      &lt;li&gt;MAGNet&lt;/li&gt;
      &lt;li&gt;HAMA&lt;/li&gt;
      &lt;li&gt;G2ANet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Partially observable setting of N agents&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131297826-23c3391a-9825-4489-9f44-bb176634e219.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM to provide hidden state in the beginning&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131298069-25b41b89-3cab-4c2c-a277-67479b0b99b8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;e()는 fully connected layer for dimension elevation&lt;/li&gt;
      &lt;li&gt;h는 hidden state from previous time step&lt;/li&gt;
      &lt;li&gt;c는 cell state from previous time step&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;m&lt;sup&gt;t(0)&lt;/sup&gt;은 message processing 전의 round 0 message를 의미.&lt;/li&gt;
  &lt;li&gt;Scheduler는 agent가 각 time step에 어떤 agent에게 message를 보내는지 결정&lt;/li&gt;
  &lt;li&gt;Message processor는 받은 message 처리를 맡음&lt;/li&gt;
  &lt;li&gt;Encoded message m&lt;sup&gt;t(l)&lt;/sup&gt;은 sub-processor l+1과 sub-scheduler l+1에 각각 forward pass 된다.&lt;/li&gt;
  &lt;li&gt;Sub-scheduler l은 adjacency matrix G&lt;sup&gt;t(l)&lt;/sup&gt;을 생성한다. 이 adjacency matrix는 directed graph로 각 agent가 각 time step t 마다 어떤 agent에게 message를 보낼지 결정한다.&lt;/li&gt;
  &lt;li&gt;Sub-processor는 sub-scheduler가 생성한 adjacency matrix 정보를 받아 integrated message set을 생성한다. 각 agent 별로 m&lt;sup&gt;t(l)&lt;/sup&gt;으로 표시된 것이 message를 뜻함.&lt;/li&gt;
  &lt;li&gt;m&lt;sup&gt;t(L)&lt;/sup&gt;일 경우, 즉 마지막 round of communication일 경우 FC layer를 통과하여 m&lt;sup&gt;t&lt;/sup&gt;을 생성한 뒤 agent policy 결정에 쓰인다.&lt;/li&gt;
  &lt;li&gt;l&amp;lt;L 일 경우 m&lt;sup&gt;t(l)&lt;/sup&gt;은 Sub-scheduler l+1과 Sub-processor l+1의 input으로 쓰인다. (뒤에 나오는 sub-scheduler에 대한 설명을 보면 sub-scheduler l+1에는 쓰이지 않는 것 같기도…)&lt;/li&gt;
  &lt;li&gt;m&lt;sup&gt;t&lt;/sup&gt;은 맨 처음 얻었던 hidden state h&lt;sup&gt;t&lt;/sup&gt;와 concatenate 되어 policy head와 value head의 input feature로 사용한다.&lt;/li&gt;
  &lt;li&gt;Policy head는 FC를 씌운 뒤 softmax를 적용한 것이고, time step t에서의 action은 softmax 함수를 통과한 뒤의 distribution에서 sampling을 통해 결정된다.&lt;/li&gt;
  &lt;li&gt;Value head는 single FC layer로 이 알고리즘의 baseline으로 사용된다. (Advantage의 value function)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Decides when each agent should send messages and whom each agent should address messages to.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131336840-cb3f9f1b-9039-4288-a613-51ea85970a47.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각각의 sub-scheduler는 m&lt;sup&gt;t(0)&lt;/sup&gt;을 input으로 받아 directed graph G&lt;sup&gt;t(l)&lt;/sup&gt;을 output으로 내놓는다.&lt;/li&gt;
  &lt;li&gt;첫번째 sub-scheduler의 경우만 GAT network를 포함하고 있다. GAT는 Graph Attention Network에 나온 구조 그대로를 사용하였다.&lt;/li&gt;
  &lt;li&gt;각각의 agent에 대한 attention score는 아래와 같이 계산한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131338335-4ef3dee8-982c-47ba-8bda-fd2a5147eeb4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Wm = (D’xD)x(Dx1) = D’x1&lt;/li&gt;
  &lt;li&gt;Concatenation -&amp;gt; 2D’x1 &amp;lt;- columnwise concatenation&lt;/li&gt;
  &lt;li&gt;a&lt;sup&gt;T&lt;/sup&gt;Wm = 1x1 &amp;lt;- a의 size: 2D’x1&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;E&lt;sup&gt;t&lt;/sup&gt;&lt;sub&gt;i,j&lt;/sub&gt; = (e&lt;sup&gt;t&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt; II e&lt;sup&gt;t&lt;/sup&gt;&lt;sub&gt;j&lt;/sub&gt;) -&amp;gt; size: 2D x 1&lt;/li&gt;
  &lt;li&gt;내 생각엔 E&lt;sup&gt;t&lt;/sup&gt; matrix는 Nx2DxN이 되야 될 것 같은데… 이건 코드 보면서 자세히 봐보도록 하자.&lt;/li&gt;
  &lt;li&gt;이런 E&lt;sup&gt;t&lt;/sup&gt;을 MLP와 Gumbel Softmax에 차례로 통과시키면 G&lt;sup&gt;t(l)&lt;/sup&gt;을 얻을 수 있다. 이는 binary value로 이루어진 그래프로 ij번째 값이 1이면 j번째 agent가 i번째 agent에게 t time step의 l번째 라운드에서 메시지를 보낸다는 소리이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;message-processor&quot;&gt;Message Processor&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input: 각 라운드의 directed graph G + 각 라운드의 encoded message m&lt;/li&gt;
  &lt;li&gt;Output: processed message m&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;구조: GAT which takes input of {m&lt;sup&gt;t(l-1)&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;}&lt;sup&gt;N&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131343400-b61b1080-7bdc-4bb4-98ce-b101f917a80b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Self-loop available&lt;/li&gt;
  &lt;li&gt;calculation of the coefficient for a standard GAT layer is a non-differential operation for the graph, using above equation allows us to retain the gradient of g&lt;sup&gt;t(l)&lt;/sup&gt;&lt;sub&gt;ij&lt;/sub&gt;.&lt;/li&gt;
  &lt;li&gt;Message Processor에서 g까지 end-to-end로 training 시키면 scheduler을 업데이트하기 위해 별도의 loss function을 정의해줘야하는 수고를 덜 수 있다.&lt;/li&gt;
  &lt;li&gt;각 round의 message는 아래와 같이 얻을 수 있다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131453231-86a8a919-96ca-4295-94bd-2d1aa54cf57c.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;training&quot;&gt;Training&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Policy network에서 FC layer와 LSTM의 parameter는 training efficiency를 위해 공유한다.&lt;/li&gt;
  &lt;li&gt;Multi-threaded synchronous multi-agent policy gradient -&amp;gt; A3C 성능이 안 좋아서 그런건가?&lt;/li&gt;
  &lt;li&gt;Policy를 학습하면서 baseline으로 사용되는 value function도 monte-carlo estimate과 최대한 가깝게 update 함.&lt;/li&gt;
  &lt;li&gt;Policy head와 value head는 parameter를 공유하지 않음.&lt;/li&gt;
  &lt;li&gt;Loss function은 아래와 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131457840-bb74f4dd-a9e2-4671-87bb-2ba6e19100dd.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Different thread는 θ와 pi를 공유해서 각각 gradient를 계산하고 batch마다 synchronously accumulate 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;pseudocode&quot;&gt;Pseudocode&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131458559-81151aa0-284a-49a4-9d91-f16e3bb6ac28.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Step 15&amp;amp;16: Determine action probability distribution from the policy and sample the action&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluation-environment&quot;&gt;Evaluation Environment&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;CommNet, IC3Net, GA-CommNet, TarMAC-IC3Net과 비교&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Predator-Prey
    &lt;ul&gt;
      &lt;li&gt;N predators with limited visions searching for a stationary prey&lt;/li&gt;
      &lt;li&gt;Predator action: up, down, left, right, stay&lt;/li&gt;
      &lt;li&gt;Predator incurs a reward -0.05 for each time step until the prey is found.&lt;/li&gt;
      &lt;li&gt;Episode is done when all the predators find the prey before a predefined maximum time limit.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131469781-7e6e9dee-12bb-40a5-a0c1-4f0d028fadd0.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131469919-75ef9578-71f8-4ff7-b128-43b2d7694f02.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;Lesser communication after agent 5 first reaches the prey at step 23.&lt;/li&gt;
      &lt;li&gt;Other agents quickly reach the prey in the following 7 steps.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Traffic Junction
    &lt;ul&gt;
      &lt;li&gt;Intersecting routes and cars with limited vision, requires communication to avoid collisions.&lt;/li&gt;
      &lt;li&gt;Cars enter the traffic junction with a probability p&lt;sub&gt;arrive&lt;/sub&gt;&lt;/li&gt;
      &lt;li&gt;Maximum number of cars in the environment at a specific time is denotes as N&lt;sub&gt;max&lt;/sub&gt;&lt;/li&gt;
      &lt;li&gt;Action is either “gas” or “brake”&lt;/li&gt;
      &lt;li&gt;The goal is to maximize the success rate (i.e. no collisions within an episode)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Andrew Fong</name></author><category term="Papers Explained" /><category term="Multi agents" /><category term="Task allocation" /><category term="Reinforcement Learning" /><category term="MARL" /><summary type="html">Paper Link: https://arxiv.org/abs/1911.10715</summary></entry><entry><title type="html">Continuous Control with Deep Reinforcement Learning (DDPG)</title><link href="https://jungwoohan72.github.io/rl%20algorithm%20replication/2021/08/27/DDPG.html" rel="alternate" type="text/html" title="Continuous Control with Deep Reinforcement Learning (DDPG)" /><published>2021-08-27T00:00:00+09:00</published><updated>2021-08-27T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/rl%20algorithm%20replication/2021/08/27/DDPG</id><content type="html" xml:base="https://jungwoohan72.github.io/rl%20algorithm%20replication/2021/08/27/DDPG.html">&lt;p&gt;Paper link: &lt;a href=&quot;https://arxiv.org/abs/1509.02971&quot;&gt;https://arxiv.org/abs/1509.02971&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DQN의 continuous action domain 버전&lt;/li&gt;
  &lt;li&gt;Off policy Actor-critic 사용&lt;/li&gt;
  &lt;li&gt;Deterministic Policy Gradient 사용&lt;/li&gt;
  &lt;li&gt;이 논문에서 제시한 알고리즘이 dynamics에 대한 full observability를 가지고 있는 planning 알고리즘과 비슷할 정도로 좋은 성능을 보임.&lt;/li&gt;
  &lt;li&gt;Raw pixel input을 받아서 Policy를 end-to-end 학습 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;DQN은 이산화되고 low-dimensional action space를 가진 문제만 풀 수 있었음. 왜냐하면 DQN의 policy 학습 자체가 행동가치함수를 최대화 하는 action을 찾는 방향으로 이루어졌기 때문.&lt;/li&gt;
  &lt;li&gt;위 같은 점이 왜 continuous domain에서 적용이 불가능한가?
    &lt;ul&gt;
      &lt;li&gt;일단 continuous domain을 이산화시키려면 무수히 많은 action space를 고려해야함. 이렇게 되면 dimension 하나가 늘어날 때마다 고려해야하는 action space의 갯수가 exponential하게 늘어나서 
  curse of dimensionality 문제를 겪게 됨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DQN에서 다음과 같은 장점들을 채택해서 actor-critic method를 stabilize하고자 함
    &lt;ol&gt;
      &lt;li&gt;Off-policy로 모은 sample들을 모아서 replay buffer 만듬. 이럴 경우 여러 에피소드에 걸쳐 모은 sample들을 학습에 사용하기 때문에 sample 간의 correlation을 최대한 줄일 수 있음.&lt;/li&gt;
      &lt;li&gt;Target Q-network를 사용.&lt;/li&gt;
      &lt;li&gt;DQN에서 사용한 트릭들 외에도 batch normalization도 사용&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;DDPG는 동일한 hyperparameter set과 network structure를 사용하여 여러 다양한 문제를 품.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;일반적인 행동가치함수는 아래와 같이 표현할 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131423574-f27e7d12-a97c-4c30-8950-0dd6a8d18f4a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Bellman Equation을 사용하여 recursive한 form으로 표현하면 다음과 같이 표현할 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131423635-49ddd9c7-24de-4093-ad74-52331d2aa40a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;만약 policy가 deterministic 하다면 더 이상 a&lt;sub&gt;t+1&lt;/sub&gt;에 따른 행동가치함수의 기댓값을 계산 하지 않아도 됨. 기댓값을 구하는 과정은 특정 action의 확률과 해당 action을 취했을 때의 행동가치함수를 곱해서 모두 더하는데, action이 결정적이면 해당 action에 대한 행동가치함수만 고려하면 됨.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131431845-f3d5f088-8544-4b8f-b380-cb53ffb364be.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이렇게 되면 Q&lt;sup&gt;μ&lt;/sup&gt;를 stochasitc behavior policy β를 통해 얻은 sample들을 통해 off-policy 학습할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Off-policy 알고리즘의 예시 중 하나로 Q-Learning을 언급하고 있기도 하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131432478-50fe47e6-d1f4-4c90-9a1c-c8d5f03e19ad.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Actor-critic approach based on the DPG algorithm&lt;/li&gt;
  &lt;li&gt;DPG 알고리즘은 actor function μ(sIθ&lt;sup&gt;μ&lt;/sup&gt;)을 사용하여 state를 특정 action으로 deterministically mapping 한다.&lt;/li&gt;
  &lt;li&gt;Actor function은 다음과 같은 policy gradient 방법을 사용하여 update 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131443050-9fb4d468-528e-487e-b6f0-0923f6e17f57.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Critic은 Bellman equation을 사용한 Q-Learning 알고리즘을 통해 학습한다.&lt;/li&gt;
  &lt;li&gt;대부분의 최적화 알고리즘이 그렇듯 neural network를 강화학습에 사용하기 위해서는 sample들이 independently and identically distributed 되어야 한다는 조건이 필요하다.
    &lt;ul&gt;
      &lt;li&gt;DQN은 replay buffer를 사용하여 이러한 문제를 해결하고자 했다.&lt;/li&gt;
      &lt;li&gt;time step마다 minibatch를 샘플링하여 actor와 critic을 업데이트 했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Q-Learning을 사용하게 되면 업데이트 하고 있는 Q function이 target network로도 사용되기 때문에 Q function이 diverge할 수도 있다는 단점이 있다.
    &lt;ul&gt;
      &lt;li&gt;이를 해결하기 위해 Q 함수를 그대로 복사해서 target network를 만드는 것이 아니라 “soft” target update를 사용한다.&lt;/li&gt;
      &lt;li&gt;Soft update는 θ’ &amp;lt;- τθ + (1-τ)θ’ with τ « 1로 표현할 수 있는데 여기서 θ‘는 업데이트 전의 actor와 critic의 parameter이다. 즉 업데이트 전과 후의 parameter를 적절히 조합하여 새로운 parameter를 얻는다는 뜻이다.&lt;/li&gt;
      &lt;li&gt;이 같은 방법을 사용하면 update를 천천히, 그리고 안정적으로 진행할 수 있다. 업데이트가 천천히 진행된다는 단점이 있을 수 있지만, 안정성 측면에서 그만큼의 효과를 내고 있다고 설명하고 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Low-dimensional feature vector observation을 사용하게 되면, 각 observation이 unit이 다르거나 scale이 다른 경우가 발생할 수 있다. 이를 해결하기 위해 batch normalization을 사용한다.
    &lt;ul&gt;
      &lt;li&gt;Minibatch의 sample들이 unit mean and variance를 가지도록 normalize&lt;/li&gt;
      &lt;li&gt;State input, all layers of μ, all layers of Q network에 normalization을 진행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Continuous dimension에서 가장 큰 문제는 exploration이다.
    &lt;ul&gt;
      &lt;li&gt;기존의 actor policy에 noise를 추가해줌으로써 exploration이 가능하다.&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131446819-45aaa56d-32fe-493f-9ff5-4570b9bae560.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;DDPG에서는 Ornstein-Uhlenbeck process를 사용해서 noise를 sample 했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pseudocode&quot;&gt;Pseudocode&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131446937-d9b5f16f-d2e7-43f4-8c1f-360a927cba92.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Low-dimensional state description (joint angles and positions)&lt;/li&gt;
  &lt;li&gt;High-dimensional renderings of the environment&lt;/li&gt;
  &lt;li&gt;For each timestep, step the simulation 3 timesteps, rendering each time.&lt;/li&gt;
  &lt;li&gt;Observation reported to the agent contains 9 feature maps (RGP of each of the 3 renderings) which allows to infer velocieis using the differences between frames.&lt;/li&gt;
  &lt;li&gt;Frames were downsampled to 64 x 64 pixels and the 8-bit RGB values were converted to floating point scaled to [0,1]&lt;/li&gt;
  &lt;li&gt;Test 시에는 explorating noise를 제외한 policy를 사용.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Andrew Fong</name></author><category term="RL Algorithm Replication" /><category term="RL" /><category term="Policy gradient" /><category term="DDPG" /><summary type="html">Paper link: https://arxiv.org/abs/1509.02971</summary></entry><entry><title type="html">Tasks with Cost Growing over Time and Agent Reallocation Delays</title><link href="https://jungwoohan72.github.io/papers%20explained/2021/08/27/tasks-with-cost-growing-over-time-and-agent-reallocation-delays.html" rel="alternate" type="text/html" title="Tasks with Cost Growing over Time and Agent Reallocation Delays" /><published>2021-08-27T00:00:00+09:00</published><updated>2021-08-27T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/papers%20explained/2021/08/27/tasks-with-cost-growing-over-time-and-agent-reallocation-delays</id><content type="html" xml:base="https://jungwoohan72.github.io/papers%20explained/2021/08/27/tasks-with-cost-growing-over-time-and-agent-reallocation-delays.html">&lt;p&gt;Multi-robot task allocation with growing completion cost and simultaneity constraints&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Completion costs change predictably over time.&lt;/li&gt;
  &lt;li&gt;Important to allocate agents to prevent tasks from growing so much that they become unsolvable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Identical homogeneous agents&lt;/li&gt;
  &lt;li&gt;Tasks with growing completion costs can become difficult or impossible to complete later.&lt;/li&gt;
  &lt;li&gt;Two famous methods for task allocation:
    &lt;ul&gt;
      &lt;li&gt;Threshold based method: agents individually assess the constraints and their ability to complete each task.&lt;/li&gt;
      &lt;li&gt;Auction based method: market inspired auction methods typically require more communication and are more centralized.
Agent with largest bidding takes the task.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Proposed method strikes a balance between distribution and centralization.
    &lt;ul&gt;
      &lt;li&gt;Each agent is directed to an area by central authority, but upon reaching the destination, agents act on their own logic.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-description&quot;&gt;Problem Description&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Agent must be on a task’s location in order to apply work.&lt;/li&gt;
  &lt;li&gt;More agents than tasks since multiple agents must be assigned to a task.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task는 다음과 같이 정의되는 cost를 가지고 있다.
&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/131081908-fd828cdf-23c0-41dc-98aa-6b59f2cb1720.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;w: work per time unit per agent&lt;/li&gt;
      &lt;li&gt;h: monotonically increasing function&lt;/li&gt;
      &lt;li&gt;n: number of agents working on task i at time t&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If h(f) &amp;gt; w x n, it means that the task is growing faster than the assigned agents can reduce it. Then, the task can never be completed.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Andrew Fong</name></author><category term="Papers Explained" /><category term="Multi agents" /><category term="Task allocation" /><summary type="html">Multi-robot task allocation with growing completion cost and simultaneity constraints</summary></entry><entry><title type="html">A taxonomy for task allocation problems with temporal and ordering constraints</title><link href="https://jungwoohan72.github.io/papers%20explained/2021/08/26/a-taxonomy-for-task-allocation-problems-with-temporal-and-ordering-constraints.html" rel="alternate" type="text/html" title="A taxonomy for task allocation problems with temporal and ordering constraints" /><published>2021-08-26T00:00:00+09:00</published><updated>2021-08-26T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/papers%20explained/2021/08/26/a-taxonomy-for-task-allocation-problems-with-temporal-and-ordering%20constraints</id><content type="html" xml:base="https://jungwoohan72.github.io/papers%20explained/2021/08/26/a-taxonomy-for-task-allocation-problems-with-temporal-and-ordering-constraints.html">&lt;p&gt;Paper link: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0921889016306157&quot;&gt;https://www.sciencedirect.com/science/article/pii/S0921889016306157&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;초심으로 돌아가자. 요즘 너무 RL에 지배당한 느낌이다.&lt;/p&gt;

&lt;p&gt;일단 Multi-robot을 운용하려면 task allocation이 필수적인데 최근 떠오른 연구 아이디어가 여러가지 constraint를 고려한 task allocation이라 비 RL 관련 논문들을 좀 찾아 보고자 한다.&lt;/p&gt;

&lt;p&gt;주로 찾아봐야 할 내용은 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Resource constraint (로봇의 resource가 정해져 있을 때의 효율적인 task allocation)&lt;/li&gt;
  &lt;li&gt;Priority considering (작업의 중요도에 따른 순차적인 task allocation)&lt;/li&gt;
  &lt;li&gt;Synchronization constraint (여러 대의 로봇이 같이 수행해야 되는 작업)&lt;/li&gt;
  &lt;li&gt;Precedence constraint (특정 task를 수행하기 전에 수행되어야 하는 task 목록)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 논문에서 얻어갈 수 있는 아이디어는 simultaneity constraint와 precedence constraint 정도인 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;multi-robot-task-allocation이란&quot;&gt;Multi-robot Task Allocation이란?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;아마존에서는 1초에 426개의 물건이 판매된다고 한다. 물건을 배송하기 위해서는 물류창고의 로봇이 주문을 받고, 물건을 꺼내오고, 포장하고, 배송 시스템에 전달을 해야한다.&lt;/li&gt;
  &lt;li&gt;과연 로봇 한대만 가지고서 많은 주문을 위 순서대로 빠르게 처리가 가능할까? 대부분 아니라고 답할 것이다.&lt;/li&gt;
  &lt;li&gt;그렇다면 여러대의 로봇을 사용해야 된다는 뜻인데, 어떤 로봇이 어떤 물건을 꺼내올 것인지 정하고, 로봇끼리 충돌 없이 움직일 수 있는 경로를 계획하는 등등 고려해야할 것이 상당히 많아진다.&lt;/li&gt;
  &lt;li&gt;이러한 문제를 해결하는 과정이 Task Allocation 과정이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;논문의-focus&quot;&gt;논문의 Focus&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Temporal constraint와 ordering constraint가 존재하는 multi-robot task allocation 문제 -&amp;gt; MRTA/TOC(Multi-Robot Task Allocation/Temporal and Ordering Constraint)
    &lt;ul&gt;
      &lt;li&gt;Temporal constraint: 특정 시점에 수행이 되어야 하는 task 고려&lt;/li&gt;
      &lt;li&gt;Ordering constraint: 특정 순서대로 수행이 되어야 하는 task 고려&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;위 constraint 등을 고려한 objective function을 최적화하는 것이 task allocation의 목적
    &lt;ul&gt;
      &lt;li&gt;Cost는 총 소요시간인 makespan이 될 수도 있다.&lt;/li&gt;
      &lt;li&gt;또 다른 옵션으로는 로봇이 움직인 거리 (traveled distance)로 설정할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;task-allocation-vs-vrp&quot;&gt;Task Allocation vs VRP&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Task allocation을 보면서 제일 먼저 든 생각은 VRP랑 다른 게 뭐지?라는 점이다. Cost를 만약 traveled distance로 하면 VRP랑 똑같아지지 않을까라는 생각이 있었는데, 이 논문에서 이런 점을 다루고 있다.&lt;/li&gt;
  &lt;li&gt;VRP와 다른점?
    &lt;ul&gt;
      &lt;li&gt;VRP는 정해진 vehicle 수가 보통 없는 반면, robotics 도메인에서의 task allocation은 가용 가능한 로봇의 수가 정해져 있다. 그리고 미션을 수행함에 따라 그 숫자가 줄어들 수 있다.&lt;/li&gt;
      &lt;li&gt;VRP의 경우 모든 vehicle이 정해진 depot에서 출발해야하고, 돌아와야 한다. 하지만 robotics 도메인에서 이러한 설정은 일반적이지 않다.&lt;/li&gt;
      &lt;li&gt;대부분의 VRP 문제는 homogeneous vehicle을 가정한다.&lt;/li&gt;
      &lt;li&gt;로봇을 사용한 task allocation 문제에서는 로봇 간의 communication이 중요하게 작용한다.
        &lt;ul&gt;
          &lt;li&gt;S.S. Ponda - Distributed Chance-Constrained Task Allocation for Autonomous Multi-Agent Teams&lt;/li&gt;
          &lt;li&gt;J. Jackson - Distributed Constrained Minimum-Time Schedules in Networks of Arbitrary Topology&lt;/li&gt;
          &lt;li&gt;T. Mercker - An Extension of Consensus-Based Auction Algorithms for Decentralized, Time-Constrained Task Assignment&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;constraints&quot;&gt;Constraints&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Time Window Constraint
    &lt;ul&gt;
      &lt;li&gt;[earliest start time, latest start time, earliest finish time, latest finish time]으로 표현되기도 한다.&lt;/li&gt;
      &lt;li&gt;위 같을 경우 time window의 lower boundary는 earliest start time이 되고, upper boundary는 latest finish time이 된다.&lt;/li&gt;
      &lt;li&gt;Deadline constraint 같은 경우는 로봇이 task가 expire되기 전에 task에 도달해야만 하는 constraint를 부여한다.&lt;/li&gt;
      &lt;li&gt;Task allocation with time window constraint는 NP-hard 문제임.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Precedence and Synchronization Constraints
    &lt;ul&gt;
      &lt;li&gt;Time window 없이 partial order 혹은 total order에만 제약 조건을 걸어줌. ‘A task 전에 B task가 반드시 수행되어야 함’과 같은 constraint&lt;/li&gt;
      &lt;li&gt;Multi-robot을 사용하게 되면 precedence/synchronization constraint에 걸쳐 있는 task들이 서로 다른 로봇에게 할당되는 경우도 생기는데, 이런 경우는 한 로봇이 다른 로봇의 미션 수행에 큰 영향을 끼칠 수 있으므로 undesirable하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard vs Soft Temporal Constraint
    &lt;ul&gt;
      &lt;li&gt;Hard: time window를 못 맞추면 utility function이 0이 됨.
        &lt;ul&gt;
          &lt;li&gt;예시: SAR (Search And Rescue) 상황에서 사람이 재난 상황에 처하고, 특정 time window 안에 구조를 하지 못하면 utility function이 0으로 drop.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Soft: time window를 못 맞추면 utility function이 exponentially 감소함. (패널티 부여)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Andrew Fong</name></author><category term="Papers Explained" /><category term="Multi agents" /><category term="Task allocation" /><summary type="html">Paper link: https://www.sciencedirect.com/science/article/pii/S0921889016306157</summary></entry><entry><title type="html">Multi-Robot Dynamic Task Allocation for Exploration and Destruction</title><link href="https://jungwoohan72.github.io/papers%20explained/2021/08/22/multi-robot-exploration-and-destruction.html" rel="alternate" type="text/html" title="Multi-Robot Dynamic Task Allocation for Exploration and Destruction" /><published>2021-08-22T00:00:00+09:00</published><updated>2021-08-22T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/papers%20explained/2021/08/22/multi-robot-exploration-and-destruction</id><content type="html" xml:base="https://jungwoohan72.github.io/papers%20explained/2021/08/22/multi-robot-exploration-and-destruction.html">&lt;p&gt;논문 링크: &lt;a href=&quot;https://link.springer.com/article/10.1007/s10846-019-01081-3&quot;&gt;https://link.springer.com/article/10.1007/s10846-019-01081-3&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-Robot Tasks Allocation (MRTA) in exploration and destruction domain where robots cooperatively search for targets hidden in the environment and attempt to destroy them.&lt;/li&gt;
  &lt;li&gt;Robots have prior knowledge about the suspicious locations, not the exact locations of the targets.&lt;/li&gt;
  &lt;li&gt;Destruction task is dynamically generated along with the execution of exploration task.&lt;/li&gt;
  &lt;li&gt;Each robot has different strike ability and each target has uncertain anti-strike ability. Either the robot or target is likely to be damaged in the destruction task.&lt;/li&gt;
  &lt;li&gt;Approach via auction-based approach, vacancy chain approach, and deep Q-learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;논문에서-강조하는-점&quot;&gt;논문에서 강조하는 점&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-agent Dynamic Task Allocation -&amp;gt; 바뀌는 환경에 따라 Task Allocation이 이뤄져야 함. 예를 들어, hidden target이 발견되면 destruction task를 진행해야 하는 것처럼 기존엔 없던 Task가 생겨서 이를 로봇에게 할당해야 함.&lt;/li&gt;
  &lt;li&gt;대부분의 MATA 문제들은 Search And Rescue (SAR)이나 Delivery 문제 같이 비교적 안전한 문제 세팅에서 이루어짐. 이 논문에서는 confrontational environment에서의 MATA 문제를 다룸.&lt;/li&gt;
  &lt;li&gt;Pure exploration이나 routing 문제 같은 것들은 MATA로 포장은 하지만 Task의 위치를 미리 알고 있는 경우에는 mTSP 문제로 generalize 시킬 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;방법론-설명&quot;&gt;방법론 설명&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Auction-based: 어떤 Task에 대해 각 robot은 ‘bidding’을 진행함. 그리고 가장 높은 ‘bidding’ 값을 제시한 robot이 해당 task를 가져감. (개인적으로는 이게 heuristic을 사용한 approach랑 다른 게 뭔지 모르겠음…)&lt;/li&gt;
  &lt;li&gt;Vacancy Chain: Multi-robot 시스템에서 수행할 task가 없는 로봇이 생기거나 unallocated task가 발견 되면 reallocation을 진행해서 비는 시간을 줄이는 방법.&lt;/li&gt;
  &lt;li&gt;Learning-based: Deep Q-Learning 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;시나리오-세팅&quot;&gt;시나리오 세팅&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;k개의 homogeneous한 로봇이 존재&lt;/li&gt;
  &lt;li&gt;각 로봇마다 P&lt;sub&gt;k&lt;/sub&gt;의 strike ability를 가지고 있음.&lt;/li&gt;
  &lt;li&gt;n개의 Hidden target이 존재.&lt;/li&gt;
  &lt;li&gt;각 target은 P&lt;sub&gt;k&lt;/sub&gt; tilde로 표기되는 anti-strike ability를 가지고 있음.&lt;/li&gt;
  &lt;li&gt;Distributed (decentralized) approach를 사용해서 각각의 robot은 local observation을 토대로 독립적인 decision making을 내림.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;시나리오-진행-flow&quot;&gt;시나리오 진행 Flow&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;로봇들은 suspicious한 target location l&lt;sub&gt;m&lt;/sub&gt; where n &amp;lt; m 을 알고 있음.&lt;/li&gt;
  &lt;li&gt;Exploration task가 먼저 수행 되는데, 모든 l&lt;sub&gt;i&lt;/sub&gt;을 방문해서 hidden target을 찾는 과정임.&lt;/li&gt;
  &lt;li&gt;모든 suspicious target location을 방문하고 나면 exploration이 종료됨.&lt;/li&gt;
  &lt;li&gt;Exploration이 끝나면 hidden target들에 대한 destruction task가 진행됨.&lt;/li&gt;
  &lt;li&gt;Target을 발견하고 나면 target의 위치는 알 수 있지만 target의 anti-strike ability는 알 수 없음.&lt;/li&gt;
  &lt;li&gt;Destruction 수행 결과는 둘중 하나이다:
    &lt;ul&gt;
      &lt;li&gt;Robot의 strike ability가 target의 anti-strike ability보다 높아서 target을 없앨 수 있는 경우&lt;/li&gt;
      &lt;li&gt;Robot의 strike ability가 target의 anti-strik abilitiy보다 낮아서 target을 없앨 수 없는 경우. 이 경우 target은 다른 robot에게 reallocate된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Destruction task는 모든 target을 없앨 때까지 진행된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;auction-based&quot;&gt;Auction-based&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130351780-1aa3ea43-0d5f-41d0-9e37-cb0d105a2f43.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Team objective는 로봇들의 total travelling cost를 최소화하는 MINISUM을 objective로 설정하고 진행됨.&lt;/li&gt;
  &lt;li&gt;Allocation을 위한 ‘bidding’은 robot의 현재 위치부터 target까지의 Euclidean distance로 estimate 된다.&lt;/li&gt;
  &lt;li&gt;Exploration task에서는 현재 로봇 위치로부터 suspicious target location까지의 거리를 사용. 가장 작은 bidding을 한 robot이 task를 가져감.&lt;/li&gt;
  &lt;li&gt;몇 번의 bidding round를 거쳐서 suspiciouss location 전부를 방문할 수 있도록 initial task allocation을 진행.&lt;/li&gt;
  &lt;li&gt;Exploration task를 진행하는 중에 target을 발견하게 되면 destruction task를 바로 진행하게 되는데, 자신의 strike ability보다 강한 target을 만나서 로봇이 손상을 입으면 그 로봇에게 할당되었던 task들은 더 이상 수행 불가하므로 다른 로봇들에게 재할당 됨.&lt;/li&gt;
  &lt;li&gt;위 같은 이유 때문에 dynamic task allocation 문제로 생각해야 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130351803-896c102f-5941-40d1-a1df-44b2bff85397.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;exploration 중간에 새로운 allocation이 발생하면 travel cost가 늘어날 수 밖에 없는데, 기존의 allocated된 task들의 순서는 바꾸지 않고, 새로운 task를 기존의 task들 사이에 끼워넣는 형태로 reallocation을 진행한다.&lt;/li&gt;
  &lt;li&gt;옵션들 중에서 가장 travelling cost가 작은 execution sequence를 채택한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-based&quot;&gt;Learning-based&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;State은 아래와 같이 정의&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130351271-95b10c01-599b-4959-84cd-6dadcb74ac3d.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;왼쪽 Matrix 같은 경우 1은 자기 자신을 뜻하고, 2는 다른 agent를 뜻한다. 그리고 음수의 값들은 task들을 의미한다.&lt;/li&gt;
      &lt;li&gt;오른쪽 Matrix 같은 경우 1은 자기 자신을 뜻하고, 파란색 점선으로 표시된 0 (allocated) 혹은 -1 (unallocated)은 task의 allocation 상태를 뜻한다. 그리고 빨간색 점선으로 표시된 양수는 해당 로봇까지의 travelling cost를 뜻한다.&lt;/li&gt;
      &lt;li&gt;이런 state representation을 사용하게 되면 map size가 변하지만 않으면 state matrix 크기를 고정시킬 수 있다는 장점이 있다.&lt;/li&gt;
      &lt;li&gt;다만 여기서 조금 애매한 점은 task나 로봇이 아닌 경우 모두 0으로 표기하는데, 이렇게 되면 allocated task나 가까이 있는 다른 agent의 표기와 겹치게 되어 학습에 문제를 줄 수도 있을 것 같다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Action은 next allocated task로 정의된다. 따라서 action space는 task의 갯수가 될 것 같다.&lt;/li&gt;
  &lt;li&gt;Action은 매 step 진행되는 것이 아니라 unallocated task가 발생하면 그때마다 action을 수행한다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reward는 아래와 같다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130351964-9906fe35-0af3-4202-89fa-3168215e7a51.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;c&lt;sub&gt;i&lt;/sub&gt;: travel cost of robot i&lt;/li&gt;
      &lt;li&gt;첫번째 항은 total travelling cost에 대한 penalty term&lt;/li&gt;
      &lt;li&gt;두번째 항은 makespan에 대한 penalty term&lt;/li&gt;
      &lt;li&gt;세번째 항은 unbalanced된 traveling cost에 대한 penalty term&lt;/li&gt;
      &lt;li&gt;네번째 항은 성공적인 allocation에 대한 reward term이다.&lt;/li&gt;
      &lt;li&gt;Reasonable 해 보이지만 조금 더 잘 정의할 수 있을 것 같은데…&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130352341-65eda282-de36-4301-9b19-a41c0a17f3e6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;10 robot / 40 suspicious locations&lt;/li&gt;
  &lt;li&gt;보면 알 수 있듯이 learning-based 방법의 성능이 아주 좋지 않다. Attention! Learn to solve routing problems 논문이 나온 해에 쓰여진 논문이라 뭔가 더 비교되는 거 같은데, 
attention 방법론을 쓰면 auction-based보다 좀 나은 성능을 기대해볼 수 있지 않을까…&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Andrew Fong</name></author><category term="Papers Explained" /><category term="Multi agents" /><category term="Task allocation" /><summary type="html">논문 링크: https://link.springer.com/article/10.1007/s10846-019-01081-3</summary></entry><entry><title type="html">Proximal Policy Optimiztion (PPO)</title><link href="https://jungwoohan72.github.io/rl%20algorithm%20replication/2021/08/19/ppo.html" rel="alternate" type="text/html" title="Proximal Policy Optimiztion (PPO)" /><published>2021-08-19T00:00:00+09:00</published><updated>2021-08-19T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/rl%20algorithm%20replication/2021/08/19/ppo</id><content type="html" xml:base="https://jungwoohan72.github.io/rl%20algorithm%20replication/2021/08/19/ppo.html">&lt;p&gt;Paper Link: &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot;&gt;https://arxiv.org/abs/1707.06347&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Policy Gradient 방법 중 하나로 실험 결과에서 대부분의 다른 Policy Gradient 보다 좋은 성능을 보임.&lt;/li&gt;
  &lt;li&gt;Sthocastic gradient ascent&lt;/li&gt;
  &lt;li&gt;다른 Policy gradient 알고리즘들은 minibatch 하나당 한번의 gradient 업데이트를 하고 끝내는 반면 PPO는 minibatch 하나를 여러번의 epoch에 걸쳐 사용하여 gradient를 업데이트 함.&lt;/li&gt;
  &lt;li&gt;TRPO에 비해서 구현이 비교적 쉬움.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;다른-policy-gradient에-비해-나은-점&quot;&gt;다른 Policy Gradient에 비해 나은 점?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Vanilla Policy Gradient 방법들은 data efficiency 측면과 안정성 측면에서 좋지 않은 모습을 보임.&lt;/li&gt;
  &lt;li&gt;TRPO 같은 경우는 내용이 너무 복잡함.&lt;/li&gt;
  &lt;li&gt;TRPO와 달리 first-order optimization을 통해 gradient update를 진행함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;policy-optimization이란&quot;&gt;Policy Optimization이란?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Advantageous Actor-Critic의 loss function은 다음과 같음. Advantage function은 보통 Q(s,a)-V(s)이다. 아래 loss function을 maximize하는 방향으로 학습이 진행.
왜 minimize가 아니고 maximize하는지 모르겠다면 REINFORCE 게시물 참조. 간단하게 말하면 아래 loss function은 value function과 같기 때문에 최대로 만들어줘야 한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130087533-d6a94f79-c982-4cd6-8f63-2df0d1cc8b0d.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;위 loss function을 따라서 on-policy 업데이트를 계속 진행하면 REINFORCE 알고리즘에서 살펴 봤듯이 학습이 굉장히 불안정하다. 논문에서는 
‘destructively large policy updates’라고 표현하고 있는데, gradient가 너무 급격하게 바뀌는 경우가 많기 때문인 것 같다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;trpo에서-차용한-loss-function&quot;&gt;TRPO에서 차용한 loss function&lt;/h1&gt;

&lt;p&gt;TRPO의 loss function (‘surrogate’ objective)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130090051-b53fb5ba-1959-4806-9c41-ed6ce695dd2b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과론적으로 위 loss function에 gradient를 취해준 값은 Advantageous Actor-Critic의 loss function에 gradient를 취해준 값과 같은데, importance sampling을 통해 같음을 증명할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130090951-96fbd280-872f-4644-98d0-718224988f90.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Importance sampling은 위와 같다. 간단히 말해서 P 분포에서 sampling한 x를 input으로 가지는 f 함수의 기댓값은 P와 다른 Q 분포에서 x를 sampling 함으로써 
구할 수 있다는 것이다.&lt;/p&gt;

&lt;p&gt;컨셉은 이렇고, 직접적으로 왜 같은지는 아래에서 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130091876-81fbafe7-b26b-4612-8be7-e4c8115bea23.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;kl이-무엇인가&quot;&gt;KL이 무엇인가?&lt;/h1&gt;

&lt;p&gt;loss function에 붙은 constraint를 살펴보면 KL이라는 것을 볼 수 있다. KL은 Kullback-Leibler divergence로 두 확률분포의 차이를 계산하는 데에 사용하는 함수이다.
즉, 업데이트 전의 θ&lt;sub&gt;old&lt;/sub&gt;와 업데이트 후의 θ의 차이나는 정도를 제한함으로써 급격한 변화를 막겠단 뜻이다. 하지만 위와 같은 constraint가 걸린 optimization 문제는
풀기에 상당히 복잡하므로 TRPO에서는 constraint form 대신 아래와 같은 penalty form을 사용해서 optimization을 진행하는 것을 제시했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130375721-21e07832-c99b-434f-8db8-169d40cf5801.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 TRPO는 위와 같은 penalty form 대신에 hard constraint form을 사용했는데, 이는 다양한 문제에 적용 가능한 하나의 β를 찾기 어려울 뿐 아니라 하나의 문제에서도 고정된 β 값은 성능이
좋지 못한 것을 발견했다.&lt;/p&gt;

&lt;h1 id=&quot;clipped-surrogate-objective&quot;&gt;Clipped Surrogate Objective&lt;/h1&gt;

&lt;p&gt;위와 같은 문제를 해결하기 위해 PPO에서는 KL divergence를 쓰는 대신에 Clipped Surrogate Objective라는 modified된 loss 함수를 제시했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130376120-7daa3180-6fc7-49ea-8817-f43bb6955e82.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 r&lt;sub&gt;t&lt;/sub&gt;(θ)는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130376377-5ee7bf08-32cf-417b-b1ef-7218c12c206a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약 ε = 0.2 라고 한다면 clip(r&lt;sub&gt;t&lt;/sub&gt;(θ), 1-ε, 1+ε)은 항상 0.8에서 1.2 사이 값을 가지게 된다. 즉 업데이트 전 policy와 업데이트 후 policy의 probability ratio를 일정 범위 안에 속하도록 고정하겠다는 의미이다.
이렇게 clip된 값과 원래의 r&lt;sub&gt;t&lt;/sub&gt;(θ) 값 중 더 작은 값을 쓰도록 하여 update가 너무 급격하게 일어나지 않게 만들어준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130378700-cca4afbf-86e2-41a7-bcdb-379ed51d2496.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 clipped surrogate objective를 사용하여 update하는 과정을 살펴보면 아래와 같다. 우선 advantage가 0보다 크다는 의미는 해당 action이 좋은 action이므로 다음 번에는 해당 action을 할 확률을 올려야 된다는 소리이다.
그래서 r 값은 증가하게 되는데 1+ε 지점에서 clip이 일어나게 된다. 아무리 정책을 좋게 만든다 하더라도 너무 급하게 바꾸지 않는다는 뜻 같다. 어찌됐던 ε이 0.2이고, advantage가 0보다 크다면 새로운 정책의 확률은
기존 정책의 확률 보다 1.2배 이상 높아지지 못한다.&lt;/p&gt;

&lt;p&gt;마찬가지로 advantage가 0보다 크다는 의미는 해당 action이 나쁜 action이므로 해당 action을 할 확률을 낮춰야 한다는 의미이다. 여기서도 똑같이 1-ε에서 clip 되고, 이보다 크게 정책을 변화 시킬 수 없다.&lt;/p&gt;

&lt;h1 id=&quot;adaptive-kl-penalty-coefficient&quot;&gt;Adaptive KL Penalty Coefficient&lt;/h1&gt;

&lt;p&gt;Clipped Surrogate Objective를 loss function으로 쓰는 게 아니라 TRPO에서 제시한 것처럼 KL penalty를 쓰되, 고정된 β 값이 아니라 adaptive한 β값을 사용한다.
하지만 논문에서 말하길, clipped surrogate objective를 쓰는 게 성능이 더 좋다고 한다. 고로 그냥 이런 게 있구나 하고 넘어가도록 하자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130473924-841d9f77-16fd-446f-acec-785f07304c4f.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;d&amp;lt;d&lt;sub&gt;targ&lt;/sub&gt;/1.5 일 경우 β를 감소시키는 것을 볼 수 있는데, d가 작다는 뜻은 기존 정책과 바뀐 정책 간의 차이가 크지 않다는 뜻이므로 penalty를 조금만 줘야한다는 뜻이 된다.
반대로 d가 너무 크면 정책이 급격하게 바뀌었다는 뜻이므로 penalty를 크게 주어 loss function을 감소(다시 짚고 가면 우린 현재 loss function을 maximize하길 원한다)시켜 반대방향(d가 작아지도록)으로 학습이 되게 한다.&lt;/p&gt;

&lt;h1 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h1&gt;

&lt;p&gt;최종적인 loss function은 아래와 같이 정의된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130555543-4f5f4450-1904-4c19-b6fc-ddc81c17b2ff.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Loss function을 다 합쳐서 학습을 진행하는 이유는 Pytorch 등에서 제공하는 automatic differentiation을 사용하기 위해서이다. 다시 한 번 강조하자면 위 loss function을 maximize하기 위한 
stochastic gradient algorithm을 사용한다. 맨 마지막 term은 entropy로 sufficient exploration을 위해 더해진 term이다.&lt;/p&gt;

&lt;h2 id=&quot;pseudocode-for-training&quot;&gt;Pseudocode for training&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130555805-972e7096-8030-41f7-bfdc-5d83fd906876.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;우선 T (episode length 보다 훨씬 작음) timestep 동안 update를 위한 sample을 모으고 advantage를 계산
  &lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130555974-572ed068-dbf3-4820-ae44-c4cf68d4b27b.png&quot; alt=&quot;image&quot; /&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/45442859/130556043-17d47b46-b891-4589-9121-b74b44522fa1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;N개의 parallel actor을 사용하여 T timestep 동안 데이터를 모음&lt;/li&gt;
  &lt;li&gt;모은 데이터에서 M 크기의 minibatch를 만들어서 K번의 epoch 동안 stochastic gradient ascent 반복함&lt;/li&gt;
  &lt;li&gt;θ&lt;sub&gt;old&lt;/sub&gt;을 업데이트 된 θ로 바꿈&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;마찬가지로 팡요랩의 소스코드를 사용.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;PPO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PPO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;put_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_prime_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_a_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done_lst&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transition&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transition&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;s_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;r_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s_prime_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;prob_a_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;done_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;done_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; \
                                              &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_prime_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; \
                                              &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob_a_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_a&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;td_target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done_mask&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;td_target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;advantage_lst&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta_t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lmbda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;advantage_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;advantage_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;advantage_lst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;pi_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gather&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prob_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# a/b == exp(log(a)-log(b))
&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;surr1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;surr2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps_clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps_clip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;advantage&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;surr1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;surr2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;smooth_l1_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;td_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Andrew Fong</name></author><category term="RL Algorithm Replication" /><category term="RL" /><category term="Policy gradient" /><category term="PPO" /><summary type="html">Paper Link: https://arxiv.org/abs/1707.06347</summary></entry><entry><title type="html">Shortest Path</title><link href="https://jungwoohan72.github.io/algorithm%20explained/2021/08/16/shortest-path.html" rel="alternate" type="text/html" title="Shortest Path" /><published>2021-08-16T00:00:00+09:00</published><updated>2021-08-16T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/algorithm%20explained/2021/08/16/shortest-path</id><content type="html" xml:base="https://jungwoohan72.github.io/algorithm%20explained/2021/08/16/shortest-path.html">&lt;h2 id=&quot;최단거리-알고리즘&quot;&gt;최단거리 알고리즘&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;그리디 알고리즘과 다이나믹 프로그래밍 알고리즘이 최단 경로 알고리즘에 그대로 적용됨.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;다이스트라&lt;/li&gt;
  &lt;li&gt;플로이드 워셜&lt;/li&gt;
  &lt;li&gt;벨만 포드&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;다익스트라-최다-경로-알고리즘&quot;&gt;다익스트라 최다 경로 알고리즘&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;음의 간선이 없을 때 정상적으로 작동.&lt;/li&gt;
  &lt;li&gt;매번 가장 비용이 적은 노드를 선택해서 임의의 과정을 반복하기 때문에 그리디 알고리즘으로 분류.&lt;/li&gt;
  &lt;li&gt;‘각 노드에 대한 현재까지의 최단 거리 정보’를 항상 1차원 리스트에 저장하며 리스트를 계속 갱신.&lt;/li&gt;
  &lt;li&gt;과정
    &lt;ol&gt;
      &lt;li&gt;출발 노드 설정&lt;/li&gt;
      &lt;li&gt;최단 거리 테이블 초기화&lt;/li&gt;
      &lt;li&gt;‘방문하지 않은 노드 중’에서 최단 거리가 가장 짧은 노드 선택&lt;/li&gt;
      &lt;li&gt;해당 노드를 거쳐 다른 노드로 가는 비용을 계산하여 최단 거리 테이블을 갱신&lt;/li&gt;
      &lt;li&gt;3번과 4번 과정을 반복&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;간단한-버전&quot;&gt;간단한 버전&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;visited&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# a에서 b로 가는 비용 c
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_smallest_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 순차탐색
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;min_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visited&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;min_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dijkstra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;visited&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 시작 노드와 연결된 노드로 가는 cost 갱신
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
        
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 나머지 노드에 대해서 가장 cost가 작은 노드를 찾고 최단거리 갱신
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_smallest_node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;visited&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;
                
&lt;span class=&quot;n&quot;&gt;dijkstra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;INFINITY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;시간 복잡도는 O(V&lt;sup&gt;2&lt;/sup&gt;). 왜냐하면 O(V)에 걸쳐서 최단 거리가 가장 짧은 노드를 선형 탐색해야 하고, 현재 노드와 연결된 노드를 매번 일일이 확인해야하기 때문.&lt;/li&gt;
  &lt;li&gt;전체 노드의 개수가 5000개 이하라면 괜찮지만, 100000개가 넘어가면 사용 불가.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;구현은-어렵지만-더-빠른-버전&quot;&gt;구현은 어렵지만 더 빠른 버전&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;최악의 경우에도 시간 복잡도 O(ElogV) 보장&lt;/li&gt;
  &lt;li&gt;최단 거리가 가장 짧은 노드를 찾기 위해서 O(V)의 시간을 소요했던 것을 개선.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;힙-자료구조&quot;&gt;힙 자료구조&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;우선순위 큐를 구현하기 위해 사용하는 자료구조 중 하나.&lt;/li&gt;
  &lt;li&gt;우선순위가 가장 높은 데이터를 가장 먼저 삭제. (Queue는 가장 먼저 삽입된 데이터를 먼저 삭제했던 것과 비슷)&lt;/li&gt;
  &lt;li&gt;heapq 사용&lt;/li&gt;
  &lt;li&gt;우선순위 값을 표현할 때는 일반적으로 정수형 자료형의 변수가 사용됨.&lt;/li&gt;
  &lt;li&gt;우선순위 큐 라이브러리에 데이터 묶음을 넣으면, 첫 번째 원소를 기준으로 우선순위를 설정.&lt;/li&gt;
  &lt;li&gt;힙 자료구조에서는 N개의 자료를 삽입(삭제)할 때 O(NlogN)의 연산이 필요. 반면 리스트를 사용하면 삽입(삭제)할 때 하나의 원소를 삽입(삭제)할 때 O(N)만큼의 시간이 걸리므로 N개 모두 삽입(삭제)하려면 O(N&lt;sup&gt;2&lt;/sup&gt;) 
만큼의 시간이 걸린다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;heapq&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stdin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;visited&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# a에서 b로 가는 비용 c
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;dijkstra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heappush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;now&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heappop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;heapq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;heappush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
                
&lt;span class=&quot;n&quot;&gt;dijkstra&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;INFINITY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;E (특정 노드에 연결된 노드의 개수)개의 원소를 우선순위 큐에 넣었다가 모두 빼내는 연산과 매우 유사. 앞에서 말했듯이 힙에 N개의 데이터를 모두 넣고, 이후에 모두 빼는 과정은 O(NlogN)이다.
따라서 간단하게 생각하면 전체 시간 복잡도는 O(ElogE)이다.&lt;/li&gt;
  &lt;li&gt;이때 중복 간선을 포함하지 않는다면, E는 V&lt;sup&gt;2&lt;/sup&gt;보다 항상 작다. 따라서 O(logE) &amp;lt; O(logV)이다.&lt;/li&gt;
  &lt;li&gt;그러므로 시간 복잡도는 O(ElogV)이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;플로이드-워셜-알고리즘&quot;&gt;플로이드 워셜 알고리즘&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;플로이드 워셜 알고리즘은 모든 지점에서 다른 모든 지점까지의 최단 경로를 모두 구해야 하는 경우에 쓰이는 알고리즘이다.&lt;/li&gt;
  &lt;li&gt;다익스트라는 단계마다 최단 거리를 가지는 노드를 하나씩 반복적으로 선택하고, 해당 노드를 거쳐 가는 경로를 확인하며, 최단 거리 테이블을 갱신.&lt;/li&gt;
  &lt;li&gt;반면, 플로이드 워셜 알고리즘은 ‘거쳐 가는 노드’를 기준으로 알고리즘을 수행. 노드가 N개라면 각각의 노드에서 O(N&lt;sup&gt;2&lt;/sup&gt;)만큼의 연산을 수행해서 ‘현재 노드를 거쳐 가는’ 모든 경로를 고려.
따라서 시간 복잡도는 O(N&lt;sup&gt;3&lt;/sup&gt;).&lt;/li&gt;
  &lt;li&gt;플로이드 워셜 알고리즘에서는 현재 확인하고 있는 노드를 제외하고, N-1개의 노드 중에서 서로 다른 노드 (A,B)쌍을 선택한다. 이후에 A -&amp;gt; 확인하고 있는 노드 -&amp;gt; B로 가는 비용을
계산한 뒤에 최단 거리를 갱신.&lt;/li&gt;
  &lt;li&gt;위 과정을 점화식으로 표현하면 D&lt;sub&gt;ab&lt;/sub&gt; = min(D&lt;sub&gt;ab&lt;/sub&gt;, D&lt;sub&gt;ak&lt;/sub&gt;+D&lt;sub&gt;kb&lt;/sub&gt;)이다.&lt;/li&gt;
  &lt;li&gt;플로이드 워셜 알고리즘은 2차원 리스트에 ‘최단 거리’ 정보를 저장해야 한다.&lt;/li&gt;
  &lt;li&gt;플로이드 워셜 알고리즘은 다이나믹 프로그래밍이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 노드 개수
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 간선 개수
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# self-connection은 0으로.
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# a에서 b로 가는 비용이 c
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Andrew Fong</name></author><category term="Algorithm Explained" /><category term="Algorithm" /><category term="Shortest path" /><category term="Djikstra" /><summary type="html">최단거리 알고리즘</summary></entry><entry><title type="html">Dynamic Programming</title><link href="https://jungwoohan72.github.io/algorithm%20explained/2021/08/15/dynamic-programming.html" rel="alternate" type="text/html" title="Dynamic Programming" /><published>2021-08-15T00:00:00+09:00</published><updated>2021-08-15T00:00:00+09:00</updated><id>https://jungwoohan72.github.io/algorithm%20explained/2021/08/15/dynamic-programming</id><content type="html" xml:base="https://jungwoohan72.github.io/algorithm%20explained/2021/08/15/dynamic-programming.html">&lt;h2 id=&quot;다이나믹-프로그래밍-dp&quot;&gt;다이나믹 프로그래밍 (DP)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;메모리 공간을 약간 더 사용하면 연산 속도를 비약적으로 증가시킬 수 있는 방법.&lt;/li&gt;
  &lt;li&gt;Top-Down과 Bottom-Up 방식이 있다.&lt;/li&gt;
  &lt;li&gt;대표적인 예로는 피보나치 수열이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fibo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fibo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fibo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;n이 커질수록 수행 시간이 기하급수적으로 늘어남. 시간보잡도는 약 O(2&lt;sup&gt;N&lt;/sup&gt;).&lt;/li&gt;
  &lt;li&gt;위 연산을 수행하다 보면 동일한 함수가 반복적으로 호출되는데, 연산 횟수를 메모이제이션 기법을 통해 줄일 수 있다.&lt;/li&gt;
  &lt;li&gt;메모이제이션이란 한 번 구한 결과를 메모리 공간에 메모해두고 같은 식을 다시 호출하면 메모한 결과를 그대로 가져오는 기법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fibo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fibo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fibo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;다이나믹 프로그래밍을 이용했을 때 피보나치 수열 알고리즘의 시간 복잡도는 O(N)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;top-down&quot;&gt;Top-Down&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;큰 문제를 해결하기 위해 작은 문제를 호출&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bottom-up&quot;&gt;Bottom-Up&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;작은 문제부터 차근차근 답을 도출&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;특정 문제를 완전 탐색 알고리즘으로 접근했을 때 시간이 매우 오래 걸리면 다이나믹 프로그래밍을 적용할 수 있는지 확인.&lt;/li&gt;
  &lt;li&gt;가능하다면 탑다운 보다는 보텀업 방식을 추천.&lt;/li&gt;
  &lt;li&gt;개인적으로 DP 문제가 제일 어려운 것 같다…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;예제&quot;&gt;예제&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;정수 X가 주어질 때 정수 X에 사용할 수 있는 연산은 다음과 같이 4가지이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;X가 5로 나누어떨어지면, 5로 나눈다.&lt;/li&gt;
  &lt;li&gt;X가 3으로 나누어떨어지면, 3으로 나눈다.&lt;/li&gt;
  &lt;li&gt;X가 2로 나누어떨어지면, 2로 나눈다.&lt;/li&gt;
  &lt;li&gt;X에서 1을 뺀다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;연산 4개를 적절히 사용해서 1을 만드려고 한다. 연산을 사용하는 횟수의 최솟값을 출력하시오.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30001&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Andrew Fong</name></author><category term="Algorithm Explained" /><category term="Algorithm" /><category term="Dynamic Programming" /><summary type="html">다이나믹 프로그래밍 (DP)</summary></entry></feed>