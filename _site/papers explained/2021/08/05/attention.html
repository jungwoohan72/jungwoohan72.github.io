<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>
    
      Attention, Learn to Solve Routing Problems! &middot; Jungwoo Han
    
  </title>

  


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css" />
  

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/favicon.png" />
<link rel="shortcut icon" href="/favicon.ico" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml" />

  <!-- Additional head bits without overriding original head -->
</head>


  <body class="post">

    <div id="sidebar">
  <header>
    <div class="site-title">
      <a href="/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        Jungwoo Han
      </a>
    </div>
    <p class="lead">Welcome</p>
  </header>
  <nav id="sidebar-nav-links">
  
    <a class="home-link "
        href="/">Home</a>
  
  

  

  


  
    
  

  
    
  

  
    
      <a class="page-link "
          href="/about.html">About Me</a>
    
  

  
    
  

  
    
  

  
    
  

  

  
    
  

  
    
  

  
    
  

  

  
    
  

  
    
  

  
    
  

  
    
  


  


  
    
  

  
    
      <a class="category-link "
          href="/category/RL_algorithm_replication.html">RL Algorithm Replication</a>
    
  

  
    
  

  
    
      <a class="category-link "
          href="/category/algorithm_explained.html">Algorithm Explained</a>
    
  

  
    
      <a class="category-link "
          href="/category/blog.html">Blog</a>
    
  

  
    
  

  

  
    
      <a class="category-link "
          href="/category/papers_explained.html">Papers Explained</a>
    
  

  
    
  

  
    
  

  

  
    
  

  
    
  

  
    
  

  
    
  


  <!-- Optional additional links to insert in sidebar nav -->
</nav>


  

  <nav id="sidebar-icon-links">
  

  <a id="subscribe-link"
     class="icon" title="Subscribe" aria-label="Subscribe"
     href="/feed.xml">
    <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <circle cx="6.18" cy="17.82" r="2.18"/>
    <path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/>
</svg>
  </a>

  
  
  
  

  
    <a id="tags-link"
       class="icon"
       title="Tags" aria-label="Tags"
       href="/tags.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
    </a>
  

  
    <a id="search-link"
       class="icon"
       title="Search" aria-label="Search"
       href="/search.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
    </a>
  

  <!-- Optional additional links to insert for icons links -->
</nav>

  <p>
  &copy; 2021.
  <a href="/LICENSE.md">MIT License.</a>
</p>

</div>

    <main class="container">
      <header>
  <h1 class="post-title">Attention, Learn to Solve Routing Problems!</h1>
</header>
<div class="content">
  <div class="post-meta">
  <span class="post-date">05 Aug 2021</span>
  <span class="post-categories">
    
      &bull;

      
      
      

      
        <a href="/category/papers_explained.html">
          Papers Explained
        </a>
      
    
  </span>
</div>


  <div class="post-body">
    <p>Paper Link: <a href="https://arxiv.org/abs/1803.08475">https://arxiv.org/abs/1803.08475</a><br />
Source Code (Official): <a href="https://github.com/wouterkool/attention-learn-to-route">https://github.com/wouterkool/attention-learn-to-route</a></p>

<h1 id="abstract">Abstract</h1>

<ol>
  <li>조합최적화 문제를 풀 때 기존의 optimal solution을 구하는 과정이 아닌, heuristic을 사용하여 sub-optimal 하지만 computational cost
측면에서 이점을 가지는 solution을 찾고자 하는 아이디어가 제시됨.</li>
  <li>본 논문에서는 Pointer Network에서 사용된 attention 개념을 토대로 조합 최적화 문제를 풀기 위한 모델을 제시하고 REINFORCE 알고리즘을 사용해 학습 시키는
방법을 제시.</li>
  <li>Travelling Salesman Problem (TSP), Vehicle Routing Problem (VRP), Orienteering Problem (OP), Prize Collecting TSP (PCTSP)
등 여러 routing 문제를 하나의 hyperparameter로 풀어서 제시한 모델의 generalizability를 강조.</li>
</ol>

<h1 id="what-is-combinatorial-optimization-조합최적화">What is Combinatorial Optimization (조합최적화)?</h1>

<p>Definition: Process of searching for maxima (or minima) of an objective function F whose domain is discrete but
large configuration space (as opposed to an N-dimensional continuous space)</p>

<p>쉽게 말하면 objective function을 최대화 하거나 loss function을 최소화하는 조합을 찾아내는 게 목적인 연구분야.</p>

<p>대표적으로 TSP가 있는데, traveling distance나 traveling time을 최소화 하면서 주어진 node들을 한번씩만 방문하고 출발지점으로 돌아오는 문제이다.</p>

<p><img src="https://user-images.githubusercontent.com/45442859/128311518-2d3cff43-ec1e-4ca9-9eae-903d25762afb.png" alt="image" /></p>

<p>조합최적화에 접근하는 방법은 크게 두가지가 있는데, exact solution, 즉 optimal한 solution을 찾기 위한 방법이 있고, heuristic을 사용하여 sub-optimal하지만
computational cost 측면에서 이점을 가지는 solution을 찾기 위한 방법이 있다. 이 논문에서 제시한 방법론은 후자에 속하는 접근 방법으로, RL을 사용하였다.</p>

<p><img src="https://user-images.githubusercontent.com/45442859/128312683-4dabaa4f-13e6-48cc-9801-a1b0ac86ff77.png" alt="image" /></p>

<p>저자는 조합최적화 문제로 분류되는 여러 routing 문제들을 풀기 위해 이 모델을 제시했는데, 이 모델을 설명하기 위해 TSP problem setting을 이용한다고 한다.
모델이 TSP 문제만을 풀 수 있는건 아니며, 문제 세팅마다 약간의 모델 수정이나 환경 세팅을 해줌으로써 다양한 routing 문제를 풀 수 있다고 한다.</p>

<h1 id="problem-setting">Problem Setting</h1>

<ul>
  <li>Problem instance <strong>s</strong> as a graph with <strong>n</strong> nodes, which are fully connected (including self-connections)</li>
  <li>Each node is represented by feature <strong>x<sub>i</sub></strong> which is coordinate of node <strong>i</strong></li>
  <li>Solution is defined as a permutation of nodes <strong>π</strong> = (π<sub>1</sub>,…,π<sub>n</sub>) where 
π<sub>t</sub> ≠ π<sub>t’</sub></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Stochastic policy for choosing next node p<sub>θ</sub>(<strong>π</strong></td>
          <td>s) = <strong>∏<sub>t=1</sub></strong> p<sub>θ</sub>(π<sub>t</sub></td>
          <td>s,<strong>π</strong><sub>1:t-1</sub>)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h1 id="attention-model">Attention Model</h1>

<ul>
  <li>Encoder produces embeddings of all input nodes.</li>
  <li>Decoder produces the sequence <strong>π</strong> of input nodes, one node at a time. Also, the decoder observes a mask to know which nodes have been visited.</li>
</ul>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128447036-ce112ed4-3a59-472d-ba62-e5ffe31c7025.png" alt="attention" width="75%" height="75%" />
</p>

<p>Attention은 seq-to-seq 모델에 많이 쓰이는데, 한 문장을 다른 언어로 번역하는 예가 대표적이다. 즉 특정 단어를 output으로 내기 위해 어떤 input 단어들에 “집중” 할 것인지 결정하는 게 attention mechanism이라고 
생각하면 될 것 같다.</p>

<h2 id="encoder">Encoder</h2>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128447359-d68e4783-8ddc-4522-95e3-d6a31f3d6863.png" alt="encoder" width="75%" height="75%" />
</p>

<ul>
  <li>Input은 각 노드의 좌표 (2-dimensional)</li>
  <li>Output은 여러 Multi-Head-Attention layer를 거친 embedding vector (128-dimensional)</li>
  <li>각 노드의 embedding과 더불어 노드들의 평균을 낸 aggregated embedding도 output으로 내줌.</li>
</ul>

<h3 id="각-attention-layer는-아래와-같이-구성">각 Attention layer는 아래와 같이 구성</h3>

<ol>
  <li>일단 Raw Input이 MLP를 거치고 나면 128-dimensional Embedding이 만들어짐. (첫번째 초록색 화살표)</li>
  <li>Embedding에 Weight matrix를 곱해서 (query, key, value) set을 만듬. Multi-Head Attention이라고 불리우는 이유는 좀 더 다양한 feature들을 고려하기 위해 (query, key, value) set을 생성할 때 
dimension을 쪼개기 때문이다. 예를 들면 Single Head Attention으로 128x128 weight matrix를 사용해 128-dimensional vector로 project 해주는 대신에 8개의 16x128 weight matrix를 사용해서 16-dimensional vector 8개를 만들어
나중에 합친다.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448124-29776d0f-6f63-42c8-a1b1-8383469d0063.png" alt="query" width="50%" height="50%" />
</p>

<ol>
  <li>기준이 되는 node의 query와 나머지 주변 node들의 key끼리 dot-product를 해줘서 compatibility를 계산. 예를 들면, 1번 노드에게 나머지 노드들이 얼마나 의미를 가지는가 하는
점수를 계산해주는 과정. 너무 멀리 떨어져 있는 node의 경우 아래와 같이 처리.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448677-58382d71-5595-4249-a494-8106ec025a9b.png" alt="MHA" width="75%" height="75%" />
</p>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448736-5aa89b09-1dc6-4d0e-b037-bacb7d209352.png" alt="u" width="50%" height="50%" />
</p>

<ol>
  <li>계산된 compatibility에 softmax function을 씌워서 normalize 시켜준 값을 attention score로 씀.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448860-8dc3d6a9-d875-4640-8118-067328e00cb2.png" alt="a" width="25%" height="25%" />
</p>

<ol>
  <li>각 attention score는 각 노드의 value vector와 곱해져서 전부 더해짐.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128448936-b8c8e0f6-c512-435a-b052-bbbc978bc3db.png" alt="h" width="25%" height="25%" />
</p>

<ol>
  <li>Multi-Head Attention인 경우 위의 h’<sub>i</sub> vector는 16x1의 크기를 가진다. 앞에서 말했듯이 이 같은 8개의 vector에 128x16 weight matrix를 곱해주어 모두 더해서 최종적으로
128x1 Embedding vector를 만들어 낸다.</li>
</ol>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128452206-936ed6eb-f3d5-413f-a2cc-4d7f1e98a835.png" alt="MHA_sig" width="75%" height="75%" />
</p>

<p>위 과정이 하나의 Attention layer에서 일어나는 일이다.</p>

<h3 id="attention-layer를-통과하고-난-다음의-feed-forward-layer는-단순하게-relu와-batch-normalization으로-이루어짐">Attention Layer를 통과하고 난 다음의 Feed-Forward Layer는 단순하게 ReLU와 Batch Normalization으로 이루어짐.</h3>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128452744-f8bc1fb8-be4e-40f4-9e4d-d10641048b59.png" alt="BN" width="75%" height="75%" />
</p>

<p align="center">
    <img src="https://user-images.githubusercontent.com/45442859/128452912-436ec81e-11eb-4c53-b978-3ec542a2e70a.png" alt="FF" width="75%" height="75%" />
</p>

<h2 id="decoder">Decoder</h2>

    



<div class="post-tags">
  
    
    <a href="/tags.html#rl">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">RL</span>
    </a>
  
    
    <a href="/tags.html#routing">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">Routing</span>
    </a>
  
</div>
  </div>

  
  <section class="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
<script>
  /**
   *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
   *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://jungwoohan72.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </section>

  <section class="related">
  <h2>Related Posts</h2>
  <ul class="posts-list">
    
      <li>
        <h3>
          <a href="/blog/2021/09/01/openai-tips.html">
            OpenAI gym tip
            <small>01 Sep 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/blog/2021/08/31/conda-tips.html">
            Conda 명령어
            <small>31 Aug 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/papers%20explained/2021/08/30/multi-agent-graph-attention-attention-communication-and-teaming.html">
            Multi-Agent Graph-Attention Communication and Teaming
            <small>30 Aug 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</section>

</div>

    </main>

    <!-- Optional footer content -->

  </body>
</html>
